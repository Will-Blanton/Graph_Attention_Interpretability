{"cells":[{"cell_type":"markdown","metadata":{"id":"-EDwIzxiyi-d"},"source":["<a href=\"https://colab.research.google.com/github/batu-el/understanding-inductive-biases-of-gnns/blob/main/notebooks/Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"nRR5bUiLRZ5a"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88446,"status":"ok","timestamp":1744042768186,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"},"user_tz":240},"id":"JLx8ARPERaaB","outputId":"1b4035e3-0474-4718-b24c-3e698665e27c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Collecting torchdata\n","  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n","Collecting torch_geometric\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dgl\n","  Downloading dgl-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (553 bytes)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata) (2.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata) (2.32.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl) (1.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata) (2025.1.31)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.3.1)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dgl-2.1.0-cp311-cp311-manylinux1_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch_geometric, nvidia-cusolver-cu12, torchdata, dgl\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed dgl-2.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch_geometric-2.6.1 torchdata-0.11.0\n","Installing PyTorch Geometric\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling other libraries\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n","Collecting lovely-tensors\n","  Downloading lovely_tensors-0.1.18-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from lovely-tensors) (2.6.0+cu124)\n","Collecting lovely-numpy>=0.2.13 (from lovely-tensors)\n","  Downloading lovely_numpy-0.2.13-py3-none-any.whl.metadata (9.6 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from lovely-numpy>=0.2.13->lovely-tensors) (2.0.2)\n","Requirement already satisfied: fastcore in /usr/local/lib/python3.11/dist-packages (from lovely-numpy>=0.2.13->lovely-tensors) (1.7.29)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from lovely-numpy>=0.2.13->lovely-tensors) (7.34.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lovely-numpy>=0.2.13->lovely-tensors) (3.10.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (4.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->lovely-tensors) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->lovely-tensors) (1.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastcore->lovely-numpy>=0.2.13->lovely-tensors) (24.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (75.2.0)\n","Collecting jedi>=0.16 (from ipython->lovely-numpy>=0.2.13->lovely-tensors)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (3.0.50)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->lovely-numpy>=0.2.13->lovely-tensors) (4.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->lovely-tensors) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (2.8.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->lovely-numpy>=0.2.13->lovely-tensors) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lovely-numpy>=0.2.13->lovely-tensors) (1.17.0)\n","Downloading lovely_tensors-0.1.18-py3-none-any.whl (19 kB)\n","Downloading lovely_numpy-0.2.13-py3-none-any.whl (24 kB)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jedi, lovely-numpy, lovely-tensors\n","Successfully installed jedi-0.19.2 lovely-numpy-0.2.13 lovely-tensors-0.1.18\n"]}],"source":["!pip install torch torchdata torch_geometric dgl\n","\n","# Install required python libraries\n","import os\n","\n","# Install PyTorch Geometric and other libraries\n","if 'IS_GRADESCOPE_ENV' not in os.environ:\n","    print(\"Installing PyTorch Geometric\")\n","    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n","    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n","    !pip install -q torch-geometric\n","    print(\"Installing other libraries\")\n","    !pip install networkx\n","    !pip install lovely-tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12907,"status":"ok","timestamp":1744042781100,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"},"user_tz":240},"id":"xMiwOILkRgEP","outputId":"ea1918c7-d34f-4637-8b36-6279f3444b79"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n","  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n","/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n","  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"]},{"output_type":"stream","name":"stdout","text":["All imports succeeded.\n","Python version 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n","PyTorch version 2.6.0+cu124\n","PyG version 2.6.1\n"]}],"source":["import os\n","import sys\n","import time\n","import math\n","import random\n","import itertools\n","from datetime import datetime\n","from typing import Mapping, Tuple, Sequence, List\n","\n","import pandas as pd\n","import networkx as nx\n","import numpy as np\n","import scipy as sp\n","\n","from tqdm.notebook import tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, LayerNorm, Module, ModuleList, Sequential\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer, MultiheadAttention\n","from torch.optim import Adam\n","\n","import torch_geometric\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.datasets import Planetoid\n","\n","import torch_geometric.transforms as T\n","from torch_geometric.utils import remove_self_loops, dense_to_sparse, to_dense_batch, to_dense_adj\n","\n","from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n","\n","# from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n","\n","import lovely_tensors as lt\n","lt.monkey_patch()\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# import warnings\n","# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","# warnings.filterwarnings(\"ignore\", category=UserWarning)\n","# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","print(\"All imports succeeded.\")\n","print(\"Python version {}\".format(sys.version))\n","print(\"PyTorch version {}\".format(torch.__version__))\n","print(\"PyG version {}\".format(torch_geometric.__version__))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1744042781110,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"},"user_tz":240},"id":"zuVTx4otRi6i","outputId":"6948a26c-2996-489e-90eb-5466efb2c9ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["All seeds set.\n"]}],"source":["# Set random seed for deterministic results\n","\n","def seed(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed(0)\n","print(\"All seeds set.\")"]},{"cell_type":"markdown","metadata":{"id":"U3NPkGk2BMlS"},"source":["# Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20455,"status":"ok","timestamp":1744042801567,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"},"user_tz":240},"id":"BzQ7e-ETBORW","outputId":"7a653619-3916-4cd9-8564-f9a9d990cd0d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n","Processing...\n","Done!\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/squirrel/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/squirrel_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/wisconsin/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/wisconsin/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/wisconsin_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/cornell/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/cornell/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/cornell_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_node_feature_label.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/new_data/texas/out1_graph_edges.txt\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_0.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_1.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_2.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_3.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_4.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_5.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_6.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_7.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_8.npz\n","Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master/splits/texas_split_0.6_0.2_9.npz\n","Processing...\n","Done!\n"]}],"source":["from torch_geometric.datasets import WebKB, WikipediaNetwork\n","\n","DATASETS = {}\n","\n","# Chamelion & Squirrel\n","# Cora & Citeseer\n","# Cornell & Texas & Wisconsin\n","\n","## Mid Size Datasets\n","# Citation Networks\n","dataset = 'Cora'\n","dataset = Planetoid('/tmp/Cora', dataset)\n","data = dataset[0]\n","DATASETS['Cora'] = data\n","dataset = 'Citeseer'\n","dataset = Planetoid('/tmp/Citeseer', dataset)\n","data = dataset[0]\n","DATASETS['Citeseer'] = data\n","# Wikipedia Pages\n","dataset = 'Chameleon'\n","dataset = WikipediaNetwork(root='/tmp/Chameleon', name='Chameleon')\n","data = dataset[0]\n","DATASETS['Chameleon'] = data\n","dataset = 'Squirrel'\n","dataset = WikipediaNetwork(root='/tmp/Squirrel', name='Squirrel')\n","data = dataset[0]\n","DATASETS['Squirrel'] = data\n","### Small Sized Datasets\n","# Web Pages\n","dataset = WebKB(root='/tmp/Wisconsin', name='Wisconsin')\n","data = dataset[0]\n","DATASETS['Wisconsin'] = data\n","dataset = WebKB(root='/tmp/Cornell', name='Cornell')\n","data = dataset[0]\n","DATASETS['Cornell'] = data\n","dataset = WebKB(root='/tmp/Texas', name='Texas')\n","data = dataset[0]\n","DATASETS['Texas'] = data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42657,"status":"ok","timestamp":1744042844226,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"},"user_tz":240},"id":"repUAiSm9l4a","outputId":"16551882-f356-4056-f3a7-eb303d077bd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MucpqUWOBR5R"},"outputs":[],"source":["# import tqdm\n","# ### Shortest Paths ###\n","# def get_shortest_path_matrix(adjacency_matrix):\n","#     graph = nx.from_numpy_array(adjacency_matrix.cpu().numpy(), create_using=nx.DiGraph)\n","#     shortest_path_matrix = nx.floyd_warshall_numpy(graph)\n","#     shortest_path_matrix = torch.tensor(shortest_path_matrix).float()\n","#     return shortest_path_matrix\n","\n","# SHORTEST_PATHS = {}\n","# for data_key in tqdm.tqdm(DATASETS):\n","#   print(data_key)\n","#   data = DATASETS[data_key]\n","#   dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n","#   dense_shortest_path_matrix = get_shortest_path_matrix(dense_adj)\n","#   SHORTEST_PATHS[data_key] = dense_shortest_path_matrix\n","\n","# ### Save the Shortest Paths\n","# import pickle\n","# with open('sp_dict.pkl', 'wb') as f:\n","#     pickle.dump(SHORTEST_PATHS, f)\n","import pickle\n","with open('drive/MyDrive/Colab Notebooks/sp_dict2.pkl', 'rb') as f:\n","# with open('drive/MyDrive/L65/shortest_paths/sp_dict.pkl', 'rb') as f:\n","    SHORTEST_PATHS = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EEl8bRVBR_o"},"outputs":[],"source":["for data_key in DATASETS:\n","  data = DATASETS[data_key]\n","  data.dense_sp_matrix = SHORTEST_PATHS[data_key]\n","  data.dense_adj = to_dense_adj(data.edge_index, max_num_nodes = data.x.shape[0])[0]\n","  data.dense_adj = data.dense_adj.cuda() + torch.eye(data.dense_adj.shape[0]).cuda()\n","  data.dense_adj[data.dense_adj == 2] = 1\n","  data = T.AddLaplacianEigenvectorPE(k = 16, attr_name = 'pos_enc')(data)\n","  DATASETS[data_key] = data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1744042851574,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"},"user_tz":240},"id":"2Z5OG64uBSCP","outputId":"5918a93d-f572-4f43-b238-5c2916971828"},"outputs":[{"output_type":"stream","name":"stdout","text":["We have 10 Masks\n","Train Ratio: 0.39992615580558777\n","Val Ratio: 0.29985228180885315\n","Test Ratio: 0.3002215623855591\n","We have 10 Masks\n","Train Ratio: 0.39975953102111816\n","Val Ratio: 0.29996994137763977\n","Test Ratio: 0.30027052760124207\n","We have 10 Masks\n","Train Ratio: 0.39964866638183594\n","Val Ratio: 0.2999560832977295\n","Test Ratio: 0.30039525032043457\n","We have 10 Masks\n","Train Ratio: 0.39992308616638184\n","Val Ratio: 0.2999423146247864\n","Test Ratio: 0.3001345992088318\n","We have 10 Masks\n","Train Ratio: 0.39840638637542725\n","Val Ratio: 0.29880478978157043\n","Test Ratio: 0.3027888536453247\n","We have 10 Masks\n","Train Ratio: 0.3989070951938629\n","Val Ratio: 0.2950819730758667\n","Test Ratio: 0.3060109317302704\n","We have 10 Masks\n","Train Ratio: 0.3989070951938629\n","Val Ratio: 0.2950819730758667\n","Test Ratio: 0.3060109317302704\n"]}],"source":["### Masks ###\n","\n","def generate_masks(num_nodes=None,num_runs=None,train_ratio=None, val_ratio=None):\n","    masks = { 'train_mask': np.zeros((num_nodes, num_runs), dtype=int),\n","              'val_mask': np.zeros((num_nodes, num_runs), dtype=int),\n","              'test_mask': np.zeros((num_nodes, num_runs), dtype=int)}\n","\n","    for run in range(num_runs):\n","        indices = np.arange(num_nodes)\n","        np.random.shuffle(indices)\n","        train_end = int(train_ratio * num_nodes)\n","        val_end = train_end + int(val_ratio * num_nodes)\n","        masks['train_mask'][indices[:train_end], run] = 1\n","        masks['val_mask'][indices[train_end:val_end], run] = 1\n","        masks['test_mask'][indices[val_end:], run] = 1\n","\n","    tensor_masks = {'train_mask': torch.tensor(masks['train_mask']),\n","                    'val_mask':torch.tensor(masks['val_mask']),\n","                    'test_mask':torch.tensor(masks['test_mask'])}\n","    return tensor_masks\n","\n","for data_key in DATASETS:\n","    data = DATASETS[data_key]\n","\n","    masks = generate_masks(num_nodes=data.x.shape[0], num_runs=10, train_ratio=0.4, val_ratio=0.3)\n","    data.train_mask = masks['train_mask'].bool()\n","    data.val_mask = masks['val_mask'].bool()\n","    data.test_mask = masks['test_mask'].bool()\n","\n","    if len(data.train_mask.shape)==1:\n","      print('Add 10 Masks')\n","    else:\n","      print('We have 10 Masks')\n","      print('Train Ratio:',(data.train_mask[:,0].sum() / len(data.train_mask[:,0])).item())\n","      print('Val Ratio:',(data.val_mask[:,0].sum() / len(data.val_mask[:,0])).item())\n","      print('Test Ratio:',(data.test_mask[:,0].sum() / len(data.test_mask[:,0])).item())"]},{"cell_type":"markdown","metadata":{"id":"tPGL9tFORJCQ"},"source":["## Table 1: Dataset Statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O34_wDE-RLre"},"outputs":[],"source":["### Table 1 ###\n","### Dataset Statistics ###\n","# import dgl\n","# Homophily_Levels = {}\n","\n","# for data_key in DATASETS:\n","#   data = DATASETS[data_key]\n","#   edge_index_tensor = torch.tensor(data.edge_index.cpu().numpy(), dtype=torch.long)\n","#   g = dgl.graph((edge_index_tensor[0], edge_index_tensor[1]), num_nodes=data.x.shape[0])\n","#   g.ndata['y'] = torch.tensor(data.y.cpu().numpy(), dtype=torch.long)\n","#   Homophily_Levels[data_key] = {'Node Homophily':dgl.node_homophily(g, g.ndata['y'])*100,\n","#                                 'Edge Homophily':dgl.edge_homophily(g, g.ndata['y'])*100,\n","#                                 'Adjusted Homophily':dgl.adjusted_homophily(g, g.ndata['y'])*100,\n","#                                 'Number of Nodes': int(g.num_nodes()),\n","#                                 'Number of Edges': int(g.num_edges())\n","#                                 }\n","# df = pd.DataFrame(Homophily_Levels).round(1)\n","# df"]},{"cell_type":"markdown","metadata":{"id":"mySSwUOF-D5-"},"source":["# Cache Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp_ZMWqX_6gg"},"outputs":[],"source":["drive_path = 'drive/MyDrive/Colab Notebooks/' #replace with the directory of the trained models"]},{"cell_type":"markdown","source":["Delete this trained file from drive and re-run from training, I don't think it saved correctly (had a problem before I bought more RAM and GPU)"],"metadata":{"id":"i1co3hib2WuZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"owj35BlLxPrr"},"outputs":[],"source":["import pickle\n","\n","NUM_LAYERS = 1\n","NUM_HEADS = 1\n","# with open('drive/MyDrive/Colab Notebooks/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'rb') as f:\n","with open(drive_path + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'rb') as f:\n","  all_stats_1L_1H = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Gg9hecYAWCo"},"outputs":[],"source":["NUM_LAYERS = 1\n","NUM_HEADS = 2\n","# with open('drive/MyDrive/Colab Notebooks/' + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'rb') as f:\n","with open(drive_path + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'rb') as f:\n","  all_stats_1L_2H = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2H6yhuHKAg7Y"},"outputs":[],"source":["NUM_LAYERS = 2\n","NUM_HEADS = 1\n","with open(drive_path + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'rb') as f:\n","# with open(my_path + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'rb') as f:\n","  all_stats_2L_1H = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6FbDQO2AjyC"},"outputs":[],"source":["NUM_LAYERS = 2\n","NUM_HEADS = 2\n","with open(drive_path + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'rb') as f:\n","# with open(my_path + f'all_stats_{NUM_LAYERS}L_{NUM_HEADS}H.pkl', 'rb') as f:\n","  all_stats_2L_2H = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hNZoN3LBlg7"},"outputs":[],"source":["def get_attns(all_stats):\n","    all_attns = {}\n","\n","    for data_key in all_stats:\n","        st_avg_attentions = []\n","        dt_avg_attentions = []\n","        dt2_avg_attentions = []\n","        run_stats = all_stats[data_key]\n","        for run_idx in run_stats:\n","            attentions = run_stats[run_idx]['attentions']\n","            st_avg_attentions.append(attentions['SparseGraphTransformerModel'])\n","            dt_avg_attentions.append(attentions['DenseGraphTransformerModel'])\n","            dt2_avg_attentions.append(attentions['DenseGraphTransformerModel_V2'])\n","        st_attentions = torch.stack(st_avg_attentions)\n","        dt_attentions = torch.stack(dt_avg_attentions)\n","        dt2_attentions = torch.stack(dt2_avg_attentions)\n","        all_attns[data_key] = {'st_avg': st_attentions.mean(axis=0),\n","                              'dt_avg': dt_attentions.mean(axis=0),\n","                              'dt2_avg': dt2_attentions.mean(axis=0)\n","                              }\n","    return all_attns\n","\n","all_attns_1L_1H = get_attns(all_stats_1L_1H)\n","all_attns_1L_2H = get_attns(all_stats_1L_2H)\n","all_attns_2L_1H = get_attns(all_stats_2L_1H)\n","all_attns_2L_2H = get_attns(all_stats_2L_2H)"]},{"cell_type":"markdown","metadata":{"id":"x-Jm4RmVYeEB"},"source":["**Combining Attention**\n","\n","*   We combine the attention matrices across heads by averaging across heads. This gives us how much a node attends to another on average.\n","\n","*   We combine the attention matrices across layers by matrix multiplying $A_{L2} A_{L1}$. This gives us how much a node attends to another across layers.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVp6zSN7CPZI"},"outputs":[],"source":["model_keys = ['st_avg', 'dt_avg', 'dt2_avg']\n","data_keys = list(DATASETS.keys())\n","A1L_1H = {data_key: {model_key: all_attns_1L_1H[data_key][model_key].mean(axis=1)[0] for model_key in model_keys} for data_key in data_keys}\n","A1L_2H = {data_key: {model_key: all_attns_1L_2H[data_key][model_key].mean(axis=1)[0]  for model_key in model_keys} for data_key in data_keys}\n","A2L_1H = {data_key: {model_key: (all_attns_2L_1H[data_key][model_key].mean(axis=1)[1] @ all_attns_2L_1H[data_key][model_key].mean(axis=1)[0]).cpu() for model_key in model_keys} for data_key in data_keys}\n","A2L_2H = {data_key: {model_key: (all_attns_2L_2H[data_key][model_key].mean(axis=1)[1] @ all_attns_2L_2H[data_key][model_key].mean(axis=1)[0]).cpu() for model_key in model_keys} for data_key in data_keys}"]},{"cell_type":"code","source":["import numpy as np\n","\n","# Compute 90th percentile threshold for each dataset and model\n","percentile_thresholds = {}\n","\n","for data_key in data_keys:\n","    percentile_thresholds[data_key] = {}\n","    for model_key in model_keys:\n","        # Extract attention values\n","        attn_values = A1L_1H[data_key][model_key].flatten().cpu().numpy()\n","        # Compute 90th percentile threshold\n","        threshold = np.percentile(attn_values, 90)\n","        percentile_thresholds[data_key][model_key] = threshold\n","# Print results\n","for data_key in data_keys:\n","    for model_key in model_keys:\n","        print(f\"90th percentile threshold for {data_key} - {model_key}: {percentile_thresholds[data_key][model_key]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FirLx7muoaYn","executionInfo":{"status":"ok","timestamp":1744043549276,"user_tz":240,"elapsed":1793,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"}},"outputId":"025e5cf3-8cbd-465e-f87d-e014b6cd25f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["90th percentile threshold for Cora - st_avg: 0.0\n","90th percentile threshold for Cora - dt_avg: 7.258294499479234e-05\n","90th percentile threshold for Cora - dt2_avg: 0.0005024674464948475\n","90th percentile threshold for Citeseer - st_avg: 0.0\n","90th percentile threshold for Citeseer - dt_avg: 5.9712510847020894e-06\n","90th percentile threshold for Citeseer - dt2_avg: 0.00035468433634378016\n","90th percentile threshold for Chameleon - st_avg: 0.0\n","90th percentile threshold for Chameleon - dt_avg: 5.9682613937184215e-05\n","90th percentile threshold for Chameleon - dt2_avg: 0.0005602996097877622\n","90th percentile threshold for Squirrel - st_avg: 0.0\n","90th percentile threshold for Squirrel - dt_avg: 0.00011890644964296371\n","90th percentile threshold for Squirrel - dt2_avg: 0.00025478401221334934\n","90th percentile threshold for Wisconsin - st_avg: 0.0\n","90th percentile threshold for Wisconsin - dt_avg: 0.00020773467258550227\n","90th percentile threshold for Wisconsin - dt2_avg: 0.006559152156114578\n","90th percentile threshold for Cornell - st_avg: 0.0\n","90th percentile threshold for Cornell - dt_avg: 0.00024275595205835998\n","90th percentile threshold for Cornell - dt2_avg: 0.012357912957668304\n","90th percentile threshold for Texas - st_avg: 0.0\n","90th percentile threshold for Texas - dt_avg: 0.0002156180125894025\n","90th percentile threshold for Texas - dt2_avg: 0.007102636154741049\n"]}]},{"cell_type":"code","source":["# import networkx as nx\n","# import torch\n","# import numpy as np\n","\n","# # Select dataset and model\n","# DATASET = \"Cora\"\n","# MODEL = \"dt2_avg\"  # Try 'dt_avg' or 'dt2_avg' as well\n","\n","# # Extract attention matrix\n","# attention_matrix = A1L_2H[DATASET][MODEL]\n","\n","# # Flatten attention values\n","# attn_values = attention_matrix.flatten().cpu().numpy()\n","\n","# # Use 99.5th percentile to filter only the strongest connections\n","# percentile_threshold = np.percentile(attn_values, 90)\n","\n","# # Alternative: Use mean + std deviation as a threshold\n","# mean_threshold = attn_values.mean() + 1.5 * attn_values.std()\n","\n","# # Choose the better threshold\n","# threshold = max(percentile_threshold, mean_threshold)\n","\n","# print(f\"Threshold for {DATASET} - {MODEL}: {threshold}\")\n","\n","# def create_graph_from_attention(attention_matrix, threshold):\n","#     \"\"\"Creates a directed graph from an attention matrix.\"\"\"\n","#     G = nx.DiGraph()\n","#     num_nodes = attention_matrix.shape[0]\n","\n","#     for i in range(num_nodes):\n","#         G.add_node(i)\n","\n","#     for i in range(num_nodes):\n","#         for j in range(num_nodes):\n","#             weight = attention_matrix[i, j].item()\n","#             if weight > threshold:\n","#                 G.add_edge(i, j, weight=weight)\n","\n","#     return G\n","\n","# # Create graph\n","# Cora_graph = create_graph_from_attention(attention_matrix, threshold)\n","\n","# # Print summary\n","# print(f\"Graph for {DATASET} - {MODEL}: {Cora_graph.number_of_nodes()} nodes, {Cora_graph.number_of_edges()} edges\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haqsqqRFjvbt","executionInfo":{"status":"ok","timestamp":1744043578886,"user_tz":240,"elapsed":29608,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"}},"outputId":"2840f569-9e90-4600-a715-767cdabf35f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Threshold for Chameleon - st_avg: 0.018336549401283264\n","Graph for Chameleon - st_avg: 2277 nodes, 25924 edges\n"]}]},{"cell_type":"code","source":["# from google.colab import drive\n","# import networkx as nx\n","# import os\n","\n","# # Mount Google Drive\n","# drive.mount('/content/drive')\n","\n","# # Define the save path in your Google Drive\n","# save_path = \"/content/drive/My Drive/Colab Notebooks/Chameleon_attention_graph.graphml\"\n","\n","# # Save the graph in GraphML format (recommended for future use)\n","# nx.write_graphml(Cora_graph, save_path)\n","\n","# print(f\"Graph saved successfully at: {save_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMrXy_i2tlLc","executionInfo":{"status":"ok","timestamp":1744043579877,"user_tz":240,"elapsed":1007,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"}},"outputId":"0b50e645-6dc0-42da-be09-ef12e7a4b4cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Graph saved successfully at: /content/drive/My Drive/Colab Notebooks/Chameleon_attention_graph.graphml\n"]}]},{"cell_type":"code","source":["import networkx as nx\n","import torch\n","import numpy as np\n","from google.colab import drive\n","import os\n","\n","\n","# Define save directory in Google Drive\n","BASE_SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/attention_graphs99.5\"\n","os.makedirs(BASE_SAVE_PATH, exist_ok=True)\n","\n","ATTN_SETS = {\n","    \"1L2H\": all_attns_1L_2H,\n","    \"2L2H\": all_attns_2L_2H\n","}\n","\n","DATASETS = [\"Cora\", \"Citeseer\", \"Chameleon\"]\n","MODELS = [\"dt_avg\", \"dt2_avg\"]  # DLB and DL models\n","\n","def create_graph_from_attention(attention_matrix, threshold):\n","    \"\"\"Creates a directed graph from an attention matrix.\"\"\"\n","    G = nx.DiGraph()\n","    num_nodes = attention_matrix.shape[0]\n","\n","    for i in range(num_nodes):\n","        G.add_node(i)\n","\n","    for i in range(num_nodes):\n","        for j in range(num_nodes):\n","            weight = attention_matrix[i, j].item()\n","            if weight > threshold:\n","                G.add_edge(i, j, weight=weight)\n","\n","    return G\n","\n","def get_threshold(attention_matrix, model_name):\n","    \"\"\"Returns threshold using different percentiles based on model.\"\"\"\n","    attn_values = attention_matrix.flatten().cpu().numpy()\n","\n","    percentile = 99.5 if model_name == \"dt2_avg\" else 90\n","    percentile_threshold = np.percentile(attn_values, percentile)\n","    mean_threshold = attn_values.mean() + 1.5 * attn_values.std()\n","\n","    return max(percentile_threshold, mean_threshold)\n","\n","for dataset in DATASETS:\n","    for model in MODELS:\n","        for config_name, attn_dict in ATTN_SETS.items():\n","            try:\n","                attention_matrix = attn_dict[dataset][model]\n","\n","                # 🔧 Fix: Average over layer/head dims if needed\n","                if attention_matrix.ndim == 4:\n","                    attention_matrix = attention_matrix.mean(dim=(0, 1))\n","                elif attention_matrix.ndim == 3:\n","                    attention_matrix = attention_matrix.mean(dim=0)\n","                elif attention_matrix.ndim != 2:\n","                    raise ValueError(f\"Unexpected attention shape: {attention_matrix.shape}\")\n","\n","                threshold = get_threshold(attention_matrix, model)\n","                G = create_graph_from_attention(attention_matrix, threshold)\n","\n","                filename = f\"{dataset}_{model}_{config_name}.graphml\"\n","                save_path = os.path.join(BASE_SAVE_PATH, filename)\n","                nx.write_graphml(G, save_path)\n","\n","                print(f\"✅ Saved {filename}: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n","            except Exception as e:\n","                print(f\"Failed to process {dataset} - {model} - {config_name}: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSCH10kNZaL4","executionInfo":{"status":"ok","timestamp":1744051182358,"user_tz":240,"elapsed":532588,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"}},"outputId":"30578e96-c071-4465-bef2-4fd69a8c0c6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Saved Cora_dt_avg_1L2H.graphml: 2708 nodes, 16072 edges\n","✅ Saved Cora_dt_avg_2L2H.graphml: 2708 nodes, 10504 edges\n","✅ Saved Cora_dt2_avg_1L2H.graphml: 2708 nodes, 36667 edges\n","✅ Saved Cora_dt2_avg_2L2H.graphml: 2708 nodes, 36667 edges\n","✅ Saved Citeseer_dt_avg_1L2H.graphml: 3327 nodes, 28646 edges\n","✅ Saved Citeseer_dt_avg_2L2H.graphml: 3327 nodes, 10147 edges\n","✅ Saved Citeseer_dt2_avg_1L2H.graphml: 3327 nodes, 55345 edges\n","✅ Saved Citeseer_dt2_avg_2L2H.graphml: 3327 nodes, 55345 edges\n","✅ Saved Chameleon_dt_avg_1L2H.graphml: 2277 nodes, 27677 edges\n","✅ Saved Chameleon_dt_avg_2L2H.graphml: 2277 nodes, 29595 edges\n","✅ Saved Chameleon_dt2_avg_1L2H.graphml: 2277 nodes, 25924 edges\n","✅ Saved Chameleon_dt2_avg_2L2H.graphml: 2277 nodes, 25924 edges\n"]}]},{"cell_type":"code","source":["import networkx as nx\n","import os\n","import numpy as np\n","import pandas as pd\n","from scipy.stats import spearmanr\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Configuration\n","BASE_PATH = \"/content/drive/My Drive/Colab Notebooks/attention_graphs99.5\"\n","ATTENTION_GRAPH_FILES = [\n","    \"Chameleon_dt_avg_1L2H.graphml\",\n","    \"Chameleon_dt_avg_2L2H.graphml\",\n","    \"Chameleon_dt2_avg_1L2H.graphml\",\n","    \"Chameleon_dt2_avg_2L2H.graphml\"\n","]\n","DATASET = \"Chameleon\"\n","OUTPUT_DIR = \"/content/drive/My Drive/Colab Notebooks/Chameleon_comparison_outputs\"\n","TOP_K = 100\n","\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# Load Chameleon using PyTorch Geometric\n","import torch\n","from torch_geometric.datasets import Planetoid, WikipediaNetwork\n","from torch_geometric.utils import to_networkx\n","\n","\n","def load_original_graph(dataset_name):\n","    if dataset_name == \"Cora\":\n","        data = Planetoid(root=\"/tmp/Cora\", name=\"Cora\")[0]\n","    elif dataset_name == \"Citeseer\":\n","        data = Planetoid(root=\"/tmp/Citeseer\", name=\"Citeseer\")[0]\n","    elif dataset_name == \"Chameleon\":\n","        data = WikipediaNetwork(root=\"/tmp/Chameleon\", name=\"chameleon\")[0]\n","    else:\n","        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n","\n","    return to_networkx(data, to_undirected=True)\n","\n","\n","# Compute centrality metrics\n","def compute_metrics(G):\n","    return {\n","        'degree': nx.degree_centrality(G),\n","        'betweenness': nx.betweenness_centrality(G, normalized=True),\n","        'closeness': nx.closeness_centrality(G),\n","        'eigenvector': nx.eigenvector_centrality(G, max_iter=500, tol=1e-02),\n","        'clustering': nx.clustering(G),\n","        'pagerank': nx.pagerank(G, alpha=0.85)\n","    }\n","\n","# Compute full metric-vs-metric correlation matrix\n","def compute_cross_metric_correlation(metrics1, metrics2, label1, label2):\n","    corr_matrix = pd.DataFrame(index=metrics1.keys(), columns=metrics2.keys())\n","    for m1 in metrics1:\n","        v1 = np.array(list(metrics1[m1].values()))\n","        for m2 in metrics2:\n","            # Match values by node IDs\n","            v2 = np.array([metrics2[m2].get(n, 0.0) for n in metrics1[m1]])\n","            corr, _ = spearmanr(v1, v2)\n","            corr_matrix.loc[m1, m2] = corr\n","    corr_matrix.index = [f\"{label1}: {m}\" for m in corr_matrix.index]\n","    corr_matrix.columns = [f\"{label2}: {m}\" for m in corr_matrix.columns]\n","    return corr_matrix.astype(float)\n","\n","# Save correlation heatmap\n","def save_corr_heatmap(corr_df, title, out_path):\n","    plt.figure(figsize=(10, 7))\n","    sns.heatmap(corr_df, annot=True, fmt=\".2f\", vmin=-1, vmax=1, cmap='coolwarm', cbar=True)\n","    plt.title(title)\n","    plt.xticks(rotation=45, ha='right')\n","    plt.yticks(rotation=0)\n","    plt.tight_layout()\n","    plt.savefig(out_path)\n","    plt.close()\n","\n","# Save top-k node overlap\n","def save_topk_overlap(metrics1, metrics2, label1, label2, out_path):\n","    def get_top_k_nodes(metric_dict, k=100):\n","        return {\n","            metric: set(sorted(metric_dict[metric].items(), key=lambda x: x[1], reverse=True)[:k])\n","            for metric in metric_dict\n","        }\n","\n","    topk_1 = get_top_k_nodes(metrics1)\n","    topk_2 = get_top_k_nodes(metrics2)\n","\n","    with open(out_path, \"w\") as f:\n","        f.write(f\"Top-100 Node Overlap Between {label1} and {label2}\\n\")\n","        for metric in metrics1:\n","            nodes_1 = {node for node, _ in topk_1[metric]}\n","            nodes_2 = {node for node, _ in topk_2[metric]}\n","            common_nodes = nodes_1 & nodes_2\n","            f.write(f\"\\n- {metric.capitalize()}:\\n\")\n","            f.write(f\"  Overlapping Nodes: {len(common_nodes)}\\n\")\n","            if common_nodes:\n","                sample = sorted(list(common_nodes))[:10]\n","                f.write(f\"  Sample Nodes: {sample} ...\\n\")\n","\n","# ----- Main Execution -----\n","original_graph = load_original_graph(DATASET)\n","original_metrics = compute_metrics(original_graph)\n","\n","for filename in ATTENTION_GRAPH_FILES:\n","    path = os.path.join(BASE_PATH, filename)\n","    if not os.path.exists(path):\n","        print(f\"Missing file: {path}\")\n","        continue\n","\n","    attention_label = filename.replace(\".graphml\", \"\")\n","    print(f\"Comparing {DATASET} to {attention_label}...\")\n","\n","    # Load attention graph\n","    G = nx.read_graphml(path)\n","    G = nx.relabel_nodes(G, lambda x: int(x))  # Ensure integer node IDs\n","    attention_metrics = compute_metrics(G)\n","\n","    # Cross-metric correlation matrix\n","    corr_df = compute_cross_metric_correlation(\n","        original_metrics, attention_metrics, DATASET, attention_label\n","    )\n","\n","    # Save heatmap\n","    heatmap_file = os.path.join(OUTPUT_DIR, f\"{attention_label}_heatmap.png\")\n","    save_corr_heatmap(corr_df, f\"{DATASET} vs {attention_label}\", heatmap_file)\n","\n","    # Save top-k overlap\n","    overlap_file = os.path.join(OUTPUT_DIR, f\"{attention_label}_topk_overlap.txt\")\n","    save_topk_overlap(original_metrics, attention_metrics, DATASET, attention_label, overlap_file)\n","\n","    print(f\"Saved: {heatmap_file}, {overlap_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyC3deXtknyY","executionInfo":{"status":"ok","timestamp":1744056788332,"user_tz":240,"elapsed":86344,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"}},"outputId":"a067625d-ecd4-4812-c962-9c9c7c670cc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Comparing Chameleon to Chameleon_dt_avg_1L2H...\n","Saved: /content/drive/My Drive/Colab Notebooks/Chameleon_comparison_outputs/Chameleon_dt_avg_1L2H_heatmap.png, /content/drive/My Drive/Colab Notebooks/Chameleon_comparison_outputs/Chameleon_dt_avg_1L2H_topk_overlap.txt\n","Comparing Chameleon to Chameleon_dt_avg_2L2H...\n","Saved: /content/drive/My Drive/Colab Notebooks/Chameleon_comparison_outputs/Chameleon_dt_avg_2L2H_heatmap.png, /content/drive/My Drive/Colab Notebooks/Chameleon_comparison_outputs/Chameleon_dt_avg_2L2H_topk_overlap.txt\n","Comparing Chameleon to Chameleon_dt2_avg_1L2H...\n","Saved: /content/drive/My Drive/Colab Notebooks/Chameleon_comparison_outputs/Chameleon_dt2_avg_1L2H_heatmap.png, /content/drive/My Drive/Colab Notebooks/Chameleon_comparison_outputs/Chameleon_dt2_avg_1L2H_topk_overlap.txt\n","Comparing Chameleon to Chameleon_dt2_avg_2L2H...\n","Saved: /content/drive/My Drive/Colab Notebooks/Chameleon_comparison_outputs/Chameleon_dt2_avg_2L2H_heatmap.png, /content/drive/My Drive/Colab Notebooks/Chameleon_comparison_outputs/Chameleon_dt2_avg_2L2H_topk_overlap.txt\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import networkx as nx\n","\n","def visualize_graph(G, layout='spring', node_size=20, edge_width=0.1):\n","    \"\"\"\n","    Visualizes the graph using a force-directed layout (spring layout by default).\n","\n","    Args:\n","        G (nx.DiGraph): The directed graph.\n","        layout (str): The layout for positioning nodes ('spring', 'circular', 'kamada_kawai', etc.).\n","        node_size (int): Size of the nodes.\n","        edge_width (float): Width of the edges.\n","    \"\"\"\n","    # Choose layout for positioning nodes\n","    if layout == 'spring':\n","        pos = nx.spring_layout(G, k=0.15, iterations=20)\n","    elif layout == 'circular':\n","        pos = nx.circular_layout(G)\n","    elif layout == 'kamada_kawai':\n","        pos = nx.kamada_kawai_layout(G)\n","    else:\n","        pos = nx.spring_layout(G)  # Default spring layout\n","\n","    # Draw the graph\n","    plt.figure(figsize=(12, 12))\n","    nx.draw(G, pos, node_size=node_size, with_labels=False, edge_color='gray', width=edge_width)\n","    plt.title(f\"Graph Visualization ({layout} layout)\")\n","    plt.show()\n","\n","# Visualize the graph\n","visualize_graph(Citeseer_graph, layout='spring')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"id":"I5Bh7iLEqIR3","executionInfo":{"status":"error","timestamp":1744043579984,"user_tz":240,"elapsed":94,"user":{"displayName":"Sal Hargis","userId":"12332389453752715640"}},"outputId":"8e843bfe-fe4e-4079-a393-a6073de9600f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Cora_graph' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-0ab0c218d880>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Visualize the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mvisualize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCora_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spring'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'Cora_graph' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"ZXuPgGbGa2Nd"},"source":["# Analysis Part I: What does attention do in different models in homophilous vs heterophilous tasks?\n"]},{"cell_type":"markdown","metadata":{"id":"-n4A3L_qRB47"},"source":["## Section 1.1: Model Accuracies"]},{"cell_type":"markdown","metadata":{"id":"670dtom5sl10"},"source":["### Table 2: Accuracy Statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNkbWu7zH06U"},"outputs":[],"source":["### Table 2 ###\n","### Accuracy Statistics ###\n","\n","import pandas as pd\n","\n","pd.set_option('display.max_columns', None)\n","model_specs = {\n","    '1L1H': all_stats_1L_1H,\n","    '1L2H': all_stats_1L_2H,\n","    '2L1H': all_stats_2L_1H,\n","    '2L2H': all_stats_2L_2H\n","}\n","\n","df_spec = {}\n","\n","for spec, all_stats in model_specs.items():\n","    all_stats_df = {}\n","    for data_key, run_stats in all_stats.items():\n","        table1 = pd.concat({\n","            key: pd.DataFrame(stats['accuracy'])\n","            for key, stats in run_stats.items()\n","        }, axis=0)\n","        train_acc = table1.xs('train_acc', level=1)\n","        table1_train = pd.concat({\n","            'mean': train_acc.groupby(level=0).mean(),\n","            'std': train_acc.groupby(level=0).std()\n","        }, axis=1)\n","        test_acc = table1.xs('test_acc', level=1)\n","        table1_test = pd.concat({\n","            'mean': test_acc.groupby(level=0).mean(),\n","            'std': test_acc.groupby(level=0).std()\n","        }, axis=1)\n","        table1_final = table1_test\n","        all_stats_df[data_key] = table1_final\n","    df_spec[spec] = pd.concat(all_stats_df, axis=1).round(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pk5kZy4WSC2n"},"outputs":[],"source":["# Ignore the rows for GCN with 2 heads\n","pd.concat(df_spec)"]},{"cell_type":"markdown","metadata":{"id":"t2sf3BHFRMan"},"source":["## Section 1.2: Do the Nodes Attend to Neighbors?"]},{"cell_type":"markdown","metadata":{"id":"xhclizcqCCvY"},"source":["### Table 3: Average Attention to Neighbors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMmGVIyLB-ei"},"outputs":[],"source":["datasets_dict = dict(DATASETS.items())\n","data_keys = ['Cora', 'Citeseer', 'Chameleon', 'Squirrel', 'Cornell', 'Texas', 'Wisconsin'] #list(DATASETS.keys())\n","metrics = ['st_avg', 'dt_avg', 'dt2_avg']\n","model_specs = {'1L1H': A1L_1H, '1L2H': A1L_2H, '2L1H': A2L_1H, '2L2H': A2L_2H }\n","\n","df_1L1H = {}\n","\n","\n","for data_key in data_keys:\n","  df_1L1H[data_key] = {}\n","  for metric in metrics:\n","    df_1L1H[data_key][metric] = {}\n","    # 1L1H\n","    all_attns= model_specs['2L2H']\n","    attn = all_attns[data_key][metric].cpu()\n","    adj = datasets_dict[data_key].dense_adj.cpu()\n","    sp = datasets_dict[data_key].dense_sp_matrix.cpu()\n","    # df_1L1H[data_key][metric] = {'Neighbors': attn[adj==1].mean().item()* 100, 'Non-neighbors': attn[adj==0].mean().item()* 100, 'Ratio': attn[adj==0].mean().item()/attn[adj==1].mean().item()}\n","    df_1L1H[data_key][metric] = attn[adj==0].mean().item()/attn[adj==1].mean().item()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYPhd6SMU0lt"},"outputs":[],"source":["# pd.concat({key: pd.DataFrame(df_1L1H[key]) for key in df_1L1H.keys()}, axis=0).rename(columns={'st_avg': 'Sparse Transformer', 'dt_avg': 'Dense Transformer wB', 'dt2_avg': 'Dense Transformer'}).round(2)\n","pd.DataFrame(df_1L1H) .round(2)"]},{"cell_type":"markdown","metadata":{"id":"bIxoWInVvMg4"},"source":["### Figure 1: Attention to Neighbors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmuRKuoe6s1Q"},"outputs":[],"source":["### Figure 1 ###\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# data_key = 'Cora'\n","# metric = 'st_avg'\n","\n","metrics = ['st_avg', 'dt_avg', 'dt2_avg']\n","metrics_labels = {'st_avg':'SL', 'dt_avg':'DLB', 'dt2_avg':'DL'}\n","model_specs = {'1L1H': A1L_1H, '1L2H': A1L_2H, '2L1H': A2L_1H, '2L2H': A2L_2H }\n","\n","DATASETS1 = {key:DATASETS[key] for key in list(DATASETS.keys())[:4]}\n","DATASETS2 = {key:DATASETS[key] for key in list(DATASETS.keys())[4:]}\n","\n","DATASET_CURR = DATASETS2\n","x_min, x_max = -0.1, 1.5\n","y_min, y_max = 0, 1.1\n","\n","fig, axes = plt.subplots(1, len(DATASET_CURR), figsize=(24, 6))\n","colors = plt.cm.viridis(np.linspace(0, 1, len(model_specs) * len(metrics)))\n","\n","\n","for idx, (ax, (data_key, data_value)) in enumerate(zip(axes, DATASET_CURR.items())):\n","    color_idx = 0\n","    add = 0\n","    for model_spec in model_specs:\n","        all_attns = model_specs[model_spec]\n","        for metric in metrics:\n","            attn = all_attns[data_key][metric].flatten()\n","            adj = data_value[\"dense_adj\"].flatten()\n","            adj[adj == 2] = 1\n","            label_ = f'{model_spec}-{metrics_labels[metric]}'\n","\n","            ax.scatter((adj.cpu() + add).numpy(), attn.cpu().numpy(), label=label_, c=colors[color_idx], marker='o')\n","            ax.set_title(f'{data_key}', fontsize=20)\n","            ax.set_xticks([0.15, 1.15])\n","            ax.set_xticklabels(['Non-Neighbor', 'Neighbor'], fontsize=20)\n","            if idx == 0:\n","                ax.set_ylabel('Attention', fontsize=20)\n","            ax.set_xlim(x_min, x_max)\n","            ax.set_ylim(y_min, y_max)\n","            ax.grid(True)\n","            color_idx += 1\n","            add += 0.025\n","\n","plt.tight_layout()\n","handles, labels = fig.gca().get_legend_handles_labels()\n","fig.legend(handles[:12], labels[:12], loc='lower center',  ncol=4, fontsize=20, bbox_to_anchor=(0.5, -0.3))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6UAam6zQYj4"},"outputs":[],"source":["# # Ensure DATASET_CURR is correct\n","# DATASET_CURR = DATASETS1\n","\n","# x_min, x_max = -0.1, 1.5\n","# y_min, y_max = 0, 1.1\n","\n","# fig, axes = plt.subplots(1, len(DATASET_CURR), figsize=(24, 6))\n","# colors = plt.cm.viridis(np.linspace(0, 1, len(model_specs) * len(metrics)))\n","\n","# for idx, (ax, (data_key, data_value)) in enumerate(zip(axes, DATASET_CURR.items())):\n","#     color_idx = 0\n","#     add = 0\n","#     for model_spec in model_specs:\n","#         all_attns = model_specs[model_spec]\n","#         for metric in metrics:\n","#             attn = all_attns[data_key][metric].flatten().cpu().numpy()\n","#             adj = data_value[\"dense_adj\"].flatten().cpu().numpy()\n","#             adj[adj == 2] = 1\n","#             label_ = f'{model_spec}-{metrics_labels[metric]}'\n","\n","#             ax.scatter(adj + add, attn, label=label_, c=[colors[color_idx]], marker='o')\n","#             ax.set_title(f'{data_key}', fontsize=20)\n","#             ax.set_xticks([0.15, 1.15])\n","#             ax.set_xticklabels(['Non-Neighbor', 'Neighbor'], fontsize=20)\n","#             if idx == 0:\n","#                 ax.set_ylabel('Attention', fontsize=20)\n","#             ax.set_xlim(x_min, x_max)\n","#             ax.set_ylim(y_min, y_max)\n","#             ax.grid(True)\n","#             color_idx += 1\n","#             add += 0.025\n","\n","# plt.tight_layout()\n","# handles, labels = fig.gca().get_legend_handles_labels()\n","# fig.legend(handles[:12], labels[:12], loc='lower center', ncol=4, fontsize=20, bbox_to_anchor=(0.5, -0.3))\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"msrEDFNlSxyP"},"source":["### Figure 2: Attention to N-hop Neighborhood: 1Layer 1Head"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YL9FN2YT4Lfj"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","metric = 'dt_avg'\n","metrics = ['st_avg', 'dt_avg', 'dt2_avg']\n","metrics_labels = {'st_avg': 'SL', 'dt_avg': 'DLB', 'dt2_avg': 'DL'}\n","\n","DATASETS1 = {key: DATASETS[key] for key in list(DATASETS.keys())[:4]}\n","DATASETS2 = {key: DATASETS[key] for key in list(DATASETS.keys())[4:]}\n","\n","model_specs = [A1L_1H]\n","\n","DATASET_CURR = DATASETS\n","\n","x_min, x_max = -0.05, 20\n","y_min, y_max = -0.05, 1.05\n","\n","for metric in metrics:\n","    for all_attns in model_specs:\n","        fig, axes = plt.subplots(1, len(DATASET_CURR), figsize=(24, 6))\n","        for ax, (data_key, data_value) in zip(axes, DATASET_CURR.items()):\n","            attn = all_attns[data_key][metric].flatten().cpu().numpy()\n","            sp = data_value[\"dense_sp_matrix\"].flatten().cpu().numpy()\n","\n","            ax.scatter(sp, attn, marker='x', color='blue', alpha=0.2)\n","            ax.set_title(f'{data_key}', fontsize=20)\n","\n","            if data_key == 'Cora':\n","                ax.set_ylabel(f'{metrics_labels[metric]} Attention', fontsize=20)\n","            ax.tick_params(axis='x', labelsize=20)\n","            ax.tick_params(axis='y', labelsize=20)\n","            ax.set_xlim(x_min, x_max)\n","            ax.set_ylim(y_min, y_max)\n","            ax.grid(True)\n","\n","        plt.tight_layout()\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"EEqHsK3EWKXv"},"source":["### [NOT INCLUDED] Expected N-Hop Attention per Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYqXbkHmWQsj"},"outputs":[],"source":["### Figure 3 ###\n","\n","from matplotlib.ticker import MaxNLocator\n","\n","DATASETS1 = {key:DATASETS[key] for key in list(DATASETS.keys())[:4]}\n","DATASETS2 = {key:DATASETS[key] for key in list(DATASETS.keys())[4:]}\n","\n","DATASET_CURR = DATASETS2\n","\n","model_specs = [A1L_1H, A1L_2H, A2L_1H, A2L_2H ]\n","\n","model_key = 'dt2_avg'\n","n_datasets = len(DATASET_CURR)\n","\n","for all_attns in model_specs:\n","  fig, axes = plt.subplots(1, n_datasets, figsize=(10 * n_datasets, 6))  # Adjust the figure size as needed\n","  for idx, (data_key, ax) in enumerate(zip(DATASET_CURR, axes.flatten())):\n","    sp_curr = DATASETS[data_key].dense_sp_matrix.cuda()\n","    sp_curr = torch.nan_to_num(sp_curr, posinf=0)\n","    attention_curr = all_attns[data_key][model_key].cpu()\n","    expected_attention = np.array([(attention_curr[i] * sp_curr[i].cpu()).sum().cpu().item() for i in range(attention_curr.shape[0])])\n","    classes = DATASETS[data_key].y.cpu().numpy()\n","\n","    # Calculate the mean expected attention for each class\n","    unique_classes = np.unique(classes)\n","    mean_attentions = [np.mean(expected_attention[classes == cls]) for cls in unique_classes]\n","\n","    # Scatter plot for individual data points\n","    ax.scatter(classes, expected_attention, color='black', s=50, label=f'Dataset {data_key} Data')\n","\n","    # Bar chart for mean expected attention\n","    ax.bar(unique_classes, mean_attentions, color='blue', alpha=0.2, label=f'Dataset {data_key} Mean')\n","\n","    ax.set_title(f'{data_key}: Classes vs. Expected Attention')\n","    ax.set_xlabel('Classes')\n","    ax.set_ylabel('Expected Attention')\n","    ax.grid(True)\n","    ax.legend(title='Legend')\n","\n","    # Ensure that only integer values are shown on the x-axis\n","  ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n","  plt.tight_layout()\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XQleyQko9T6o"},"source":["# Analysis Part II: A framework to analyze a model with multiple heads and layers"]},{"cell_type":"markdown","metadata":{"id":"Sr3TdFp3DnPh"},"source":["**Combining Attention**\n","\n","*   We combine the attention matrices across heads by averaging across heads. This gives us how much a node attends to another on average.\n","\n","*   We combine the attention matrices across layers by matrix multiplying $A_{L2} A_{L1}$. This gives us how much a node attends to another across layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGImu1hDDojL"},"outputs":[],"source":["### Combined Attention Matrices ###\n","\n","model_keys = ['st_avg', 'dt_avg', 'dt2_avg']\n","data_keys = list(DATASETS.keys())\n","A1L_1H = {data_key: {model_key: all_attns_1L_1H[data_key][model_key].mean(axis=1)[0] for model_key in model_keys} for data_key in data_keys}\n","A1L_2H = {data_key: {model_key: all_attns_1L_2H[data_key][model_key].mean(axis=1)[0]  for model_key in model_keys} for data_key in data_keys}\n","A2L_1H = {data_key: {model_key: (all_attns_2L_1H[data_key][model_key].mean(axis=1)[1] @ all_attns_2L_1H[data_key][model_key].mean(axis=1)[0]).cpu() for model_key in model_keys} for data_key in data_keys}\n","A2L_2H = {data_key: {model_key: (all_attns_2L_2H[data_key][model_key].mean(axis=1)[1] @ all_attns_2L_2H[data_key][model_key].mean(axis=1)[0]).cpu() for model_key in model_keys} for data_key in data_keys}"]},{"cell_type":"markdown","metadata":{"id":"PGtk_5tlYUux"},"source":["## Section 2.1: Do the heads learn the same patterns?"]},{"cell_type":"markdown","metadata":{"id":"MmzPf6HDaG1a"},"source":["### [NOT INCLUDED] N-Hop Neighborhood Attendance Comparison for 2 Heads (1L2H Model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdytKnmjdlAK"},"outputs":[],"source":["### Figure 2 ###\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","num_heads = 2\n","\n","# data_key = 'Cora'\n","metric = 'dt2_avg'\n","\n","# Figure 2 subplots\n","DATASETS1 = {key:DATASETS[key] for key in list(DATASETS.keys())[:4]}\n","DATASETS2 = {key:DATASETS[key] for key in list(DATASETS.keys())[4:]}\n","\n","DATASET_CURR = DATASETS2\n","\n","all_attns = all_attns_2L_2H\n","x_min, x_max = 0, 10\n","y_min, y_max = 0, 0.5\n","\n","fig, axes = plt.subplots(1, len(DATASET_CURR), figsize=(24, 6))\n","for ax, (data_key, data_value) in zip(axes, DATASET_CURR.items()):\n","    attn = all_attns[data_key][metric][0]\n","    sp = data_value[\"dense_sp_matrix\"]\n","    for head_idx in range(num_heads):\n","      attn = all_attns[data_key][metric][0][head_idx]\n","      print(sp.cpu().shape,  attn.cpu().shape)\n","      ax.scatter(sp.cpu()+0.1*head_idx, attn.cpu())\n","    # ax.scatter(sp.cpu(), attn.cpu(), marker='x', color='blue')#,c=attn.cpu(), cmap='icefire')\n","    ax.set_title(f'{data_key}: Attention Paid to N-hop Neighborhood')\n","    ax.set_xlabel('N-Hop Neighborhood')\n","    ax.set_ylabel('Attention')\n","    ax.set_xlim(x_min, x_max)\n","    ax.set_ylim(y_min, y_max)\n","    ax.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-wB7KRgOaRNC"},"source":["### Figure 3: Comparison of Head Attention Patterns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uQ2h4tZZ93d"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","data_keys = ['Cora', 'Citeseer', 'Chameleon', 'Squirrel', 'Cornell', 'Texas', 'Wisconsin']\n","metrics = ['st_avg', 'dt_avg', 'dt2_avg']\n","metrics_labels = {'st_avg': 'SL', 'dt_avg': 'DLB', 'dt2_avg': 'DL'}\n","\n","x_min, x_max = 0, 1\n","y_min, y_max = 0, 1\n","\n","fig, axs = plt.subplots(len(metrics), len(data_keys), figsize=(20, 9))\n","\n","for i, model_name in enumerate(metrics):\n","    for j, dataset_name in enumerate(data_keys):\n","        head_1 = all_attns_1L_2H[dataset_name][model_name][0][0].cpu().numpy()\n","        head_2 = all_attns_1L_2H[dataset_name][model_name][0][1].cpu().numpy()\n","\n","        axs[i, j].scatter(head_1, head_2, color='black', marker='x', alpha=0.2)\n","\n","        if i == 0:\n","            axs[i, j].set_title(dataset_name, fontsize=16)\n","\n","        if j == 0:\n","            axs[i, j].set_ylabel(f'{metrics_labels[model_name]}\\nHead 2', fontsize=14)\n","\n","        if i == len(metrics) - 1:\n","            axs[i, j].set_xlabel('Head 1', fontsize=14)\n","\n","        axs[i, j].set_xticks([])\n","        axs[i, j].set_yticks([])\n","\n","        axs[i, j].set_xlim(x_min, x_max)\n","        axs[i, j].set_ylim(y_min, y_max)\n","        axs[i, j].grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GmWOH3Gza_Wr"},"source":["## Section 2.2: Do the layers learn the same pattern?"]},{"cell_type":"markdown","metadata":{"id":"c8I0FPbNl530"},"source":["### Figure 4: Comparison of Layer Attention Patterns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k73lsImRbFJ7"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","data_keys = ['Cora', 'Citeseer', 'Chameleon', 'Squirrel', 'Cornell', 'Texas', 'Wisconsin']\n","metrics = ['st_avg', 'dt_avg', 'dt2_avg']\n","metrics_labels = {'st_avg': 'SL', 'dt_avg': 'DLB', 'dt2_avg': 'DL'}\n","\n","x_min, x_max = 0, 1\n","y_min, y_max = 0, 1\n","\n","fig, axs = plt.subplots(len(metrics), len(data_keys), figsize=(20, 9))\n","\n","for i, model_name in enumerate(metrics):\n","    for j, dataset_name in enumerate(data_keys):\n","        layer1 = all_attns_2L_1H[dataset_name][model_name][0][0].cpu().numpy()\n","        layer2 = all_attns_2L_1H[dataset_name][model_name][1][0].cpu().numpy()\n","\n","        axs[i, j].scatter(layer1, layer2, color='black', marker='x', alpha=0.2)\n","\n","        if i == 0:\n","            axs[i, j].set_title(dataset_name, fontsize=16)\n","\n","        if j == 0:\n","            axs[i, j].set_ylabel(f'{metrics_labels[model_name]}\\nLayer 2', fontsize=14)\n","\n","        if i == len(metrics) - 1:\n","            axs[i, j].set_xlabel('Layer 1', fontsize=14)\n","\n","        axs[i, j].set_xticks([])\n","        axs[i, j].set_yticks([])\n","\n","        axs[i, j].set_xlim(x_min, x_max)\n","        axs[i, j].set_ylim(y_min, y_max)\n","        axs[i, j].grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ggcim6sfbNVM"},"source":["## [NOT INCLUDED] Section 2.3: Do the Models learn the same pattern?"]},{"cell_type":"markdown","metadata":{"id":"-qSuOgCfoooC"},"source":["### [NOT INCLUDED] Model Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZg_JZNWbT5G"},"outputs":[],"source":["data_keys = ['Cora', 'Citeseer', 'Chameleon', 'Squirrel', 'Cornell', 'Texas', 'Wisconsin'] #list(DATASETS.keys())\n","metrics = ['st_avg', 'dt_avg', 'dt2_avg']\n","metrics_labels = {'st_avg':'SparseT', 'dt_avg':'DenseTwB', 'dt2_avg':'DenseT'}\n","model_specs ={'1L1H': A1L_1H, '2L2H': A2L_2H} #{'1L1H': A1L_1H, '1L2H': A1L_2H, '2L1H': A2L_1H, '2L2H': A2L_2H }\n","\n","for metric in metrics:\n","  for dataset_name in data_keys:\n","      plt.title(dataset_name)\n","      plt.scatter(A1L_1H[dataset_name][metric].cpu(), A2L_2H[dataset_name][metric].cpu())\n","      plt.show()"]},{"cell_type":"markdown","metadata":{"id":"V8UI2MFF9vUq"},"source":["# Analysis Part III: Does the  graph structure in attention recover the original graph structure?"]},{"cell_type":"markdown","metadata":{"id":"Z_CmFztXGn_B"},"source":["## Section 3.1: Combining Attention Matrices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJSUn3BXuzQ6"},"outputs":[],"source":["### Combined Attention Matrices ###\n","\n","data_keys = list(DATASETS.keys())\n","A1L_1H = {data_key: {model_key: all_attns_1L_1H[data_key][model_key].mean(axis=1)[0] for model_key in model_keys} for data_key in data_keys}\n","A1L_2H = {data_key: {model_key: all_attns_1L_2H[data_key][model_key].mean(axis=1)[0]  for model_key in model_keys} for data_key in data_keys}\n","A2L_1H = {data_key: {model_key: (all_attns_2L_1H[data_key][model_key].mean(axis=1)[1] @ all_attns_2L_1H[data_key][model_key].mean(axis=1)[0]).cpu() for model_key in model_keys} for data_key in data_keys}\n","A2L_2H = {data_key: {model_key: (all_attns_2L_2H[data_key][model_key].mean(axis=1)[1] @ all_attns_2L_2H[data_key][model_key].mean(axis=1)[0]).cpu() for model_key in model_keys} for data_key in data_keys}"]},{"cell_type":"markdown","metadata":{"id":"AAqGw4O6GxCI"},"source":["## Section 3.2: Selecting a Threshold [Commented Out]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wu_kbnqCCFzr"},"outputs":[],"source":["# import numpy as np\n","\n","# model_keys = ['dt_avg', 'dt2_avg']\n","# data_keys = ['Cora', 'Citeseer', 'Chameleon', 'Squirrel', 'Cornell', 'Texas', 'Wisconsin']\n","# model_specs = {'1L1H': A1L_1H, '1L2H': A1L_2H, '2L1H': A2L_1H, '2L2H': A2L_2H }\n","\n","# thresholds = np.arange(0, 1, 0.001)  # Example range of thresholds\n","# selected_thresholds = {}\n","\n","# for spec_key in model_specs:\n","#   print(spec_key)\n","#   spec = model_specs[spec_key]\n","#   selected_thresholds[spec_key] = {}\n","#   for data_key in data_keys:\n","#       selected_thresholds[spec_key][data_key] = {}\n","#       for model_key in model_keys:\n","#           adj = DATASETS[data_key].dense_adj.cpu()\n","#           attn_base = spec[data_key][model_key].cpu()\n","\n","#           best_threshold = 0\n","#           min_difference = float('inf')\n","\n","#           for threshold in thresholds:\n","#               attn = attn_base.clone()\n","#               attn[attn >= threshold] = 1\n","#               attn[attn < threshold] = 0\n","\n","#               difference = abs(adj.sum() - attn.sum())\n","#               if difference < min_difference:\n","#                   min_difference = difference\n","#                   best_threshold = threshold\n","\n","#           # Now, best_threshold is the threshold where the difference between sums is minimized.\n","#           print(f\"Best threshold {data_key} - {model_key}: {best_threshold}\")\n","#           print(f\"Difference: {min_difference}\")\n","#           selected_thresholds[spec_key][data_key][model_key] = best_threshold\n","\n","##############\n","### Output ###\n","##############\n","\n","# 1L1H\n","# Best threshold Cora - dt_avg: 0.037\n","# Difference: 87.0\n","# Best threshold Cora - dt2_avg: 0.019\n","# Difference: 470.0\n","# Best threshold Citeseer - dt_avg: 0.06\n","# Difference: 33.0\n","# Best threshold Citeseer - dt2_avg: 0.02\n","# Difference: 743.0\n","# Best threshold Chameleon - dt_avg: 0.006\n","# Difference: 243.0\n","# Best threshold Chameleon - dt2_avg: 0.006\n","# Difference: 2515.0\n","# Best threshold Squirrel - dt_avg: 0.003\n","# Difference: 3929.0\n","# Best threshold Squirrel - dt2_avg: 0.001\n","# Difference: 86327.0\n","# Best threshold Cornell - dt_avg: 0.079\n","# Difference: 2.0\n","# Best threshold Cornell - dt2_avg: 0.06\n","# Difference: 1.0\n","# Best threshold Texas - dt_avg: 0.039\n","# Difference: 4.0\n","# Best threshold Texas - dt2_avg: 0.07200000000000001\n","# Difference: 4.0\n","# Best threshold Wisconsin - dt_avg: 0.058\n","# Difference: 5.0\n","# Best threshold Wisconsin - dt2_avg: 0.051000000000000004\n","# Difference: 10.0\n","# 1L2H\n","# Best threshold Cora - dt_avg: 0.024\n","# Difference: 39.0\n","# Best threshold Cora - dt2_avg: 0.015\n","# Difference: 560.0\n","# Best threshold Citeseer - dt_avg: 0.054\n","# Difference: 69.0\n","# Best threshold Citeseer - dt2_avg: 0.012\n","# Difference: 1171.0\n","# Best threshold Chameleon - dt_avg: 0.005\n","# Difference: 505.0\n","# Best threshold Chameleon - dt2_avg: 0.006\n","# Difference: 4235.0\n","# Best threshold Squirrel - dt_avg: 0.003\n","# Difference: 10409.0\n","# Best threshold Squirrel - dt2_avg: 0.001\n","# Difference: 147306.0\n","# Best threshold Cornell - dt_avg: 0.049\n","# Difference: 2.0\n","# Best threshold Cornell - dt2_avg: 0.054\n","# Difference: 13.0\n","# Best threshold Texas - dt_avg: 0.034\n","# Difference: 3.0\n","# Best threshold Texas - dt2_avg: 0.059000000000000004\n","# Difference: 0.0\n","# Best threshold Wisconsin - dt_avg: 0.049\n","# Difference: 3.0\n","# Best threshold Wisconsin - dt2_avg: 0.052000000000000005\n","# Difference: 23.0\n","# 2L1H\n","# Best threshold Cora - dt_avg: 0.038\n","# Difference: 98.0\n","# Best threshold Cora - dt2_avg: 0.008\n","# Difference: 801.0\n","# Best threshold Citeseer - dt_avg: 0.044\n","# Difference: 29.0\n","# Best threshold Citeseer - dt2_avg: 0.007\n","# Difference: 1769.0\n","# Best threshold Chameleon - dt_avg: 0.014\n","# Difference: 825.0\n","# Best threshold Chameleon - dt2_avg: 0.005\n","# Difference: 2650.0\n","# Best threshold Squirrel - dt_avg: 0.005\n","# Difference: 33406.0\n","# Best threshold Squirrel - dt2_avg: 0.001\n","# Difference: 8669.0\n","# Best threshold Cornell - dt_avg: 0.08\n","# Difference: 0.0\n","# Best threshold Cornell - dt2_avg: 0.05\n","# Difference: 7.0\n","# Best threshold Texas - dt_avg: 0.056\n","# Difference: 2.0\n","# Best threshold Texas - dt2_avg: 0.036000000000000004\n","# Difference: 9.0\n","# Best threshold Wisconsin - dt_avg: 0.046\n","# Difference: 2.0\n","# Best threshold Wisconsin - dt2_avg: 0.065\n","# Difference: 13.0\n","# 2L2H\n","# Best threshold Cora - dt_avg: 0.039\n","# Difference: 42.0\n","# Best threshold Cora - dt2_avg: 0.007\n","# Difference: 2419.0\n","# Best threshold Citeseer - dt_avg: 0.041\n","# Difference: 101.0\n","# Best threshold Citeseer - dt2_avg: 0.006\n","# Difference: 856.0\n","# Best threshold Chameleon - dt_avg: 0.013000000000000001\n","# Difference: 432.0\n","# Best threshold Chameleon - dt2_avg: 0.004\n","# Difference: 4218.0\n","# Best threshold Squirrel - dt_avg: 0.005\n","# Difference: 39171.0\n","# Best threshold Squirrel - dt2_avg: 0.001\n","# Difference: 22364.0\n","# Best threshold Cornell - dt_avg: 0.052000000000000005\n","# Difference: 4.0\n","# Best threshold Cornell - dt2_avg: 0.048\n","# Difference: 9.0\n","# Best threshold Texas - dt_avg: 0.02\n","# Difference: 4.0\n","# Best threshold Texas - dt2_avg: 0.047\n","# Difference: 32.0\n","# Best threshold Wisconsin - dt_avg: 0.057\n","# Difference: 2.0\n","# Best threshold Wisconsin - dt2_avg: 0.048\n","# Difference: 1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"We7bwUeB5A9P"},"outputs":[],"source":["# Save the thresholds\n","# import json\n","# file_path = 'drive/MyDrive/Colab Notebooks/L65/selected_thresholds_dict.pkl'\n","# with open(file_path, 'w') as file:\n","#     json.dump(selected_thresholds, file)"]},{"cell_type":"markdown","metadata":{"id":"TmrBNdT16a1r"},"source":["## Section 3.3: Analyzing Thresholded Attention"]},{"cell_type":"markdown","metadata":{"id":"HV66uCmFJu7o"},"source":["### Figure 5: Attention Heatmaps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"293iqERp56lO"},"outputs":[],"source":["import json\n","# Load the thresholds\n","file_path = 'drive/MyDrive/Colab Notebooks/selected_thresholds_dict.pkl'\n","with open(file_path, 'r') as file:\n","    selected_thresholds = json.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbvnfCxk7uFo"},"outputs":[],"source":["model_keys = ['dt_avg', 'dt2_avg']\n","metrics_labels = {'st_avg':'SL', 'dt_avg':'DLB', 'dt2_avg':'DL'}\n","data_keys = ['Cora', 'Citeseer', 'Chameleon', 'Squirrel', 'Cornell', 'Texas', 'Wisconsin'] #list(DATASETS.keys())\n","model_specs = {'1L1H': A1L_1H, '1L2H': A1L_2H, '2L1H': A2L_1H, '2L2H': A2L_2H }\n","\n","thresholded_attentions = {}\n","\n","thresholded_attentions = {}\n","for spec_key in model_specs:\n","  spec = model_specs[spec_key]\n","  print(spec_key)\n","  thresholded_attentions[spec_key] = {}\n","  for data_key in data_keys:\n","    thresholded_attentions[spec_key][data_key] = {}\n","    for model_key in model_keys:\n","      threshold = selected_thresholds[spec_key][data_key][model_key]\n","      attn = spec[data_key][model_key].cpu()\n","      attn[attn>=threshold] = 1\n","      attn[attn<threshold] = 0\n","      thresholded_attentions[spec_key][data_key][metrics_labels[model_key]] = attn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBJkNor48eWa"},"outputs":[],"source":["thresholded_attentions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjRg-ikU8z5e"},"outputs":[],"source":["model_keys = ['dt_avg', 'dt2_avg']\n","metrics_labels = {'st_avg':'SL', 'dt_avg':'DLB', 'dt2_avg':'DL'}\n","data_keys = ['Cora', 'Citeseer', 'Chameleon', 'Squirrel', 'Cornell', 'Texas', 'Wisconsin'] #list(DATASETS.keys())\n","model_specs = {'1L1H': A1L_1H, '1L2H': A1L_2H, '2L1H': A2L_1H, '2L2H': A2L_2H }\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","fig, axs = plt.subplots(1, 7, figsize=(14, 4))\n","\n","for j in range(7):\n","    data = DATASETS[data_keys[j]].dense_adj.cpu()\n","    axs[j].imshow(data, cmap='hot', interpolation='nearest')\n","    axs[j].set_title(data_keys[j], fontsize=15)\n","    if j == 0:\n","      axs[j].text(-0.1, 0.5, 'Adjacency', transform=axs[j].transAxes, va='center', ha='right', rotation=90, fontsize=15)\n","    axs[j].axis('off')  # Turn off the axis\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QymQ-O2qBSPV"},"outputs":[],"source":["spec_key = '1L1H'\n","fig, axs = plt.subplots(2, 7, figsize=(14, 4))\n","\n","for i in range(2):\n","    for j in range(7):\n","        data = thresholded_attentions[spec_key][data_keys[j]][metrics_labels[model_keys[i]]]\n","        axs[i, j].imshow(data, cmap='hot', interpolation='nearest')\n","        axs[i, j].axis('off')  # Turn off the axis\n","        if j == 0:\n","            axs[i, j].text(-0.1, 0.5, f'{spec_key}\\n{metrics_labels[model_keys[i]]}',\n","                           transform=axs[i, j].transAxes, va='center', ha='right',\n","                           rotation=90, fontsize=15)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nuzFjAVuBCs_"},"outputs":[],"source":["spec_key = '1L2H'\n","fig, axs = plt.subplots(2, 7, figsize=(14, 4))\n","for i in range(2):\n","    for j in range(7):\n","        data = thresholded_attentions[spec_key][data_keys[j]][metrics_labels[model_keys[i]]]\n","        axs[i, j].imshow(data, cmap='hot', interpolation='nearest')\n","        axs[i, j].axis('off')  # Turn off the axis\n","        if j == 0:\n","            axs[i, j].text(-0.1, 0.5, f'{spec_key}\\n{metrics_labels[model_keys[i]]}',\n","                           transform=axs[i, j].transAxes, va='center', ha='right',\n","                           rotation=90, fontsize=15)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WeJCKBKxBEAh"},"outputs":[],"source":["spec_key = '2L1H'\n","fig, axs = plt.subplots(2, 7, figsize=(14, 4))\n","for i in range(2):\n","    for j in range(7):\n","        data = thresholded_attentions[spec_key][data_keys[j]][metrics_labels[model_keys[i]]]\n","        axs[i, j].imshow(data, cmap='hot', interpolation='nearest')\n","        axs[i, j].axis('off')  # Turn off the axis\n","        if j == 0:\n","            axs[i, j].text(-0.1, 0.5, f'{spec_key}\\n{metrics_labels[model_keys[i]]}',\n","                           transform=axs[i, j].transAxes, va='center', ha='right',\n","                           rotation=90, fontsize=15)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXNsctzuBFUg"},"outputs":[],"source":["spec_key = '2L2H'\n","fig, axs = plt.subplots(2, 7, figsize=(14, 4))\n","for i in range(2):\n","    for j in range(7):\n","        data = thresholded_attentions[spec_key][data_keys[j]][metrics_labels[model_keys[i]]]\n","        axs[i, j].imshow(data, cmap='hot', interpolation='nearest')\n","        axs[i, j].axis('off')  # Turn off the axis\n","        if j == 0:\n","            axs[i, j].text(-0.1, 0.5, f'{spec_key}\\n{metrics_labels[model_keys[i]]}',\n","                           transform=axs[i, j].transAxes, va='center', ha='right',\n","                           rotation=90, fontsize=15)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jbDTl_aLJ0zR"},"source":["### Table 4: Adjacency Recovery: P, R, F1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"951Y3Is8u4o2"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\n","\n","model_keys = ['dt_avg', 'dt2_avg']\n","metrics_labels = {'st_avg':'SparseT', 'dt_avg':'DenseTwB', 'dt2_avg':'DenseT'}\n","data_keys = ['Cora', 'Citeseer', 'Chameleon', 'Squirrel', 'Cornell', 'Texas', 'Wisconsin'] #list(DATASETS.keys())\n","model_specs = {'1L1H': A1L_1H, '1L2H': A1L_2H, '2L1H': A2L_1H, '2L2H': A2L_2H }\n","\n","results = {}\n","for spec_key in model_specs:\n","  spec = model_specs[spec_key]\n","  print(spec_key)\n","  results[spec_key] = {}\n","  for data_key in data_keys:\n","    results[spec_key][data_key] = {}\n","    for model_key in model_keys:\n","      attn = thresholded_attentions[spec_key][data_key][metrics_labels[model_key]]\n","      adj =  DATASETS[data_key].dense_adj.cpu()\n","      f1 = f1_score(adj.flatten(), (attn >= 1).flatten())\n","      p = precision_score(adj.flatten(), (attn >= 1).flatten())\n","      r = recall_score(adj.flatten(), (attn >= 1).flatten())\n","      # Simple silarity metrics between true adjacency matrix and learnt adjacency matrix (attn)\n","      print(\"F1 Score: {:.4f}\".format(f1_score(adj.flatten(), (attn >= 1).flatten())))\n","      print(\"Precision: {:.4f}\".format(precision_score(adj.flatten(), (attn >= 1).flatten())))\n","      print(\"Recall: {:.4f}\".format(recall_score(adj.flatten(), (attn >= 1).flatten())))\n","      results[spec_key][data_key][metrics_labels[model_key]] = {'P': p , 'R' : r, 'F1': f1}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y70Xepuc6P3g"},"outputs":[],"source":["R_1L1H = results['1L1H']\n","R_1L2H = results['1L2H']\n","R_2L1H = results['2L1H']\n","R_2L2H = results['2L2H']\n","\n","R_1L1H_df = pd.concat({dataset:pd.DataFrame(R_1L1H[dataset]) for dataset in R_1L1H.keys()}, axis=1)\n","R_1L2H_df = pd.concat({dataset:pd.DataFrame(R_1L2H[dataset]) for dataset in R_1L2H.keys()}, axis=1)\n","R_2L1H_df = pd.concat({dataset:pd.DataFrame(R_2L1H[dataset]) for dataset in R_2L1H.keys()}, axis=1)\n","R_2L2H_df = pd.concat({dataset:pd.DataFrame(R_2L2H[dataset]) for dataset in R_2L2H.keys()}, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_lc83-9FX38"},"outputs":[],"source":["pd.concat({'1L1H':R_1L1H_df, '1L2H':R_1L2H_df, '2L1H':R_2L1H_df, '2L2H':R_2L2H_df}).round(4) * 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XAYmba_BGUIg"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["nRR5bUiLRZ5a","U3NPkGk2BMlS","mySSwUOF-D5-","ZXuPgGbGa2Nd","-n4A3L_qRB47","670dtom5sl10","t2sf3BHFRMan","xhclizcqCCvY","bIxoWInVvMg4","msrEDFNlSxyP","EEqHsK3EWKXv","XQleyQko9T6o","PGtk_5tlYUux","MmzPf6HDaG1a","-wB7KRgOaRNC","GmWOH3Gza_Wr","c8I0FPbNl530","ggcim6sfbNVM","-qSuOgCfoooC","V8UI2MFF9vUq","Z_CmFztXGn_B","AAqGw4O6GxCI","TmrBNdT16a1r","HV66uCmFJu7o","jbDTl_aLJ0zR"],"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"https://github.com/batu-el/understanding-inductive-biases-of-gnns/blob/main/notebooks/Analysis.ipynb","timestamp":1743702880974}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}