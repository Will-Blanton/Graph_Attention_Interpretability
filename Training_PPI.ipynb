{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "tVe55J-aw7aC",
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:17.003563Z",
     "start_time": "2025-04-26T20:12:17.000559Z"
    }
   },
   "source": [
    "# Combining Attention values across heads - Avg\n",
    "# Combining Attention values across layers - Matrix Multiply"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRR5bUiLRZ5a"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95820,
     "status": "ok",
     "timestamp": 1714340656785,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "JLx8ARPERaaB",
    "outputId": "39a34a44-a10f-4423-81a8-ca4638185152",
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:17.060810Z",
     "start_time": "2025-04-26T20:12:17.056101Z"
    }
   },
   "source": [
    "# !pip install dgl torch_geometric torch\n",
    "\n",
    "# Install required python libraries\n",
    "import os\n",
    "\n",
    "# Install PyTorch Geometric and other libraries\n",
    "# if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "#     print(\"Installing PyTorch Geometric\")\n",
    "#     !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "#     !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "#     !pip install -q torch-geometric\n",
    "#     print(\"Installing other libraries\")\n",
    "#     !pip install networkx\n",
    "#     !pip install lovely-tensors"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7222,
     "status": "ok",
     "timestamp": 1714340664004,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "xMiwOILkRgEP",
    "outputId": "1e83443e-9aaf-4622-b4c0-33f3f2f246dd",
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:19.939321Z",
     "start_time": "2025-04-26T20:12:17.060810Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from typing import Mapping, Tuple, Sequence, List\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, LayerNorm, Module, ModuleList, Sequential\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, MultiheadAttention\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import remove_self_loops, dense_to_sparse, to_dense_batch, to_dense_adj\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
    "\n",
    "# from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print(\"All imports succeeded.\")\n",
    "print(\"Python version {}\".format(sys.version))\n",
    "print(\"PyTorch version {}\".format(torch.__version__))\n",
    "print(\"PyG version {}\".format(torch_geometric.__version__))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PyCharmProjects\\Graph_Attention_Interpretability\\.venv\\Lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "D:\\PyCharmProjects\\Graph_Attention_Interpretability\\.venv\\Lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports succeeded.\n",
      "Python version 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]\n",
      "PyTorch version 2.6.0+cu118\n",
      "PyG version 2.6.1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714340664004,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "zuVTx4otRi6i",
    "outputId": "3413a2f1-30a2-413b-a451-9ad18d8e2eee",
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:19.946332Z",
     "start_time": "2025-04-26T20:12:19.940323Z"
    }
   },
   "source": [
    "# Set random seed for deterministic results\n",
    "\n",
    "def seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed(0)\n",
    "print(\"All seeds set.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All seeds set.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:19.953498Z",
     "start_time": "2025-04-26T20:12:19.946332Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Cuda available: {}\".format(torch.cuda.is_available()))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6R5GR7hRdEI"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:20.004757Z",
     "start_time": "2025-04-26T20:12:19.954501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use igraph because much faster than networkx\n",
    "import igraph as ig\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_position_info(dataset, pe_dim=16):\n",
    "    \n",
    "    processed_dataset = []\n",
    "    pe_transform = T.AddLaplacianEigenvectorPE(k=pe_dim, attr_name='pos_enc')\n",
    "\n",
    "    for data in tqdm(dataset, total=len(dataset), desc='Processing disjoint graphs'):\n",
    "        num_nodes = data.num_nodes\n",
    "        edge_index = data.edge_index\n",
    "\n",
    "        # Convert to igraph\n",
    "        edges = edge_index.t().tolist()\n",
    "        g = ig.Graph(n=num_nodes, edges=edges, directed=False)\n",
    "\n",
    "        # compute shortest path distances\n",
    "        sp_matrix = torch.tensor(g.distances(algorithm=\"johnson\"), dtype=torch.float16)\n",
    "        sp_matrix[torch.isinf(sp_matrix)] = 0\n",
    "        data.dense_sp_matrix = sp_matrix\n",
    "\n",
    "        dense_adj = to_dense_adj(edge_index, max_num_nodes=num_nodes)[0]\n",
    "        dense_adj = dense_adj + torch.eye(num_nodes, dtype=dense_adj.dtype)\n",
    "        dense_adj[dense_adj == 2] = 1  # remove double self-loops\n",
    "        data.dense_adj = dense_adj\n",
    "\n",
    "        # add Laplacian eigenvectors as positional encoding\n",
    "        data = pe_transform(data)\n",
    "\n",
    "        processed_dataset.append(data)\n",
    "\n",
    "    return processed_dataset"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44347,
     "status": "ok",
     "timestamp": 1714340708349,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "8PVfelu6ReJe",
    "outputId": "f7970243-ac97-43f7-ff9b-4fea824e91fa",
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:37.475177Z",
     "start_time": "2025-04-26T20:12:20.004757Z"
    }
   },
   "source": [
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train = add_position_info(PPI(root='/tmp/PPI', split='train'))\n",
    "val = add_position_info(PPI(root='/tmp/PPI', split='val'))\n",
    "test = add_position_info(PPI(root='/tmp/PPI', split='test'))\n",
    "\n",
    "train = DataLoader(train, batch_size=1, shuffle=True)\n",
    "val = DataLoader(val, batch_size=1)\n",
    "test = DataLoader(test, batch_size=1)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing disjoint graphs: 100%|██████████| 20/20 [00:13<00:00,  1.52it/s]\n",
      "Processing disjoint graphs: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "Processing disjoint graphs: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:37.487118Z",
     "start_time": "2025-04-26T20:12:37.475686Z"
    }
   },
   "cell_type": "code",
   "source": "data = next(iter(train))",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:37.491541Z",
     "start_time": "2025-04-26T20:12:37.487118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ex_spd = data.dense_sp_matrix\n",
    "ex_spd.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3021, 3021])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:37.495546Z",
     "start_time": "2025-04-26T20:12:37.491541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_memory_bytes = ex_spd.element_size() * ex_spd.numel()\n",
    "tensor_memory_MB = tensor_memory_bytes / (1024 ** 2)  # Convert to MB\n",
    "print(f\"Tensor memory usage: {tensor_memory_MB:.2f} MB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor memory usage: 17.41 MB\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:37.499150Z",
     "start_time": "2025-04-26T20:12:37.495546Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[3021, 50], edge_index=[2, 91338], y=[3021, 121], dense_sp_matrix=[3021, 3021], dense_adj=[3021, 3021], pos_enc=[3021, 16], batch=[3021], ptr=[2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPGL9tFORJCQ"
   },
   "source": [
    "## Table 1: Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1714340709202,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "O34_wDE-RLre",
    "ExecuteTime": {
     "end_time": "2025-04-26T20:12:37.502843Z",
     "start_time": "2025-04-26T20:12:37.500153Z"
    }
   },
   "source": [
    "# ### Table 1 ###\n",
    "# ### Dataset Statistics ###\n",
    "# import dgl\n",
    "# Homophily_Levels = []\n",
    "# \n",
    "# for data in train:\n",
    "#   edge_index_tensor = torch.tensor(data.edge_index.cpu().numpy(), dtype=torch.long)\n",
    "#   g = dgl.graph((edge_index_tensor[0], edge_index_tensor[1]), num_nodes=data.x.shape[0])\n",
    "#   g.ndata['y'] = torch.tensor(data.y.cpu().numpy(), dtype=torch.long)\n",
    "#   Homophily_Levels.append({'Node Homophily':dgl.node_homophily(g, g.ndata['y'])*100,\n",
    "#                                 'Edge Homophily':dgl.edge_homophily(g, g.ndata['y'])*100,\n",
    "#                                 'Adjusted Homophily':dgl.adjusted_homophily(g, g.ndata['y'])*100,\n",
    "#                                 'Number of Nodes': int(g.num_nodes()),\n",
    "#                                 'Number of Edges': int(g.num_edges())\n",
    "#                                 })\n",
    "# df = pd.DataFrame(Homophily_Levels).round(1)\n",
    "# df"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JV4ZtidSENR"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AKL9b7tCSDDd",
    "ExecuteTime": {
     "end_time": "2025-04-27T00:18:15.974519Z",
     "start_time": "2025-04-27T00:18:15.973517Z"
    }
   },
   "source": [
    "# PyG example code: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py\n",
    "\n",
    "class GNNModel(Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int = data.x.shape[-1],\n",
    "            hidden_dim: int = HIDDEN,\n",
    "            num_heads: int = 1,\n",
    "            num_layers: int = 1,\n",
    "            out_dim: int = len(data.y.unique()),\n",
    "            dropout: float = 0.5,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, hidden_dim)\n",
    "        self.lin_out = Linear(hidden_dim, out_dim)\n",
    "        self.layers = ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            self.layers.append(\n",
    "                GCNConv(hidden_dim, hidden_dim)\n",
    "            )\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.lin_in(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # conv -> activation ->  dropout -> residual\n",
    "            x_in = x\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = x_in + x\n",
    "\n",
    "        x = self.lin_out(x)\n",
    "        # return x.log_softmax(dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SparseGraphTransformerModel(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int = data.x.shape[-1],\n",
    "            hidden_dim: int = HIDDEN,\n",
    "            num_heads: int = 1,\n",
    "            num_layers: int = 1,\n",
    "            out_dim: int = len(data.y.unique()),\n",
    "            dropout: float = 0.5,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, hidden_dim)\n",
    "        self.lin_out = Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.layers = ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.layers.append(\n",
    "                MultiheadAttention(\n",
    "                    embed_dim = hidden_dim,\n",
    "                    num_heads = num_heads,\n",
    "                    dropout = dropout\n",
    "                )\n",
    "            )\n",
    "        self.dropout = dropout\n",
    "        self.save_attn = False\n",
    "\n",
    "    def forward(self, x, dense_adj):\n",
    "\n",
    "        x = self.lin_in(x)\n",
    "\n",
    "        self.attn_weights_list = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_in = x\n",
    "            x, attn_weights = layer(\n",
    "                x, x, x,\n",
    "                attn_mask = ~dense_adj.bool(),\n",
    "                average_attn_weights = True # the paper already averages so may as well just average here...\n",
    "            )\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = x_in + x\n",
    "\n",
    "            if self.save_attn:\n",
    "                self.attn_weights_list.append(attn_weights)\n",
    "\n",
    "        x = self.lin_out(x)\n",
    "\n",
    "        # return x.log_softmax(dim=-1)\n",
    "        return x\n",
    "\n",
    "class DenseGraphTransformerModel(Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int = data.x.shape[-1],\n",
    "            pos_enc_dim: int = 16,\n",
    "            hidden_dim: int = HIDDEN,\n",
    "            num_heads: int = 1,\n",
    "            num_layers: int = 1,\n",
    "            out_dim: int = len(data.y.unique()),\n",
    "            dropout: float = 0.5,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, hidden_dim)\n",
    "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
    "        self.lin_out = Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.layers = ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.layers.append(\n",
    "                MultiheadAttention(\n",
    "                    embed_dim = hidden_dim,\n",
    "                    num_heads = num_heads,\n",
    "                    dropout = dropout\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
    "        self.dropout = dropout\n",
    "        self.save_attn = False\n",
    "\n",
    "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
    "\n",
    "        # x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
    "        x = self.lin_in(x)  # no node positional encoding\n",
    "\n",
    "        # attention bias\n",
    "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
    "        # diagonals -> self connection, set to 0\n",
    "        # disconnected nodes -> -1\n",
    "        attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
    "            (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
    "            nan=0, posinf=0, neginf=0)\n",
    "        #attn_bias = torch.ones_like(attn_bias)\n",
    "\n",
    "        # TransformerEncoder\n",
    "        # x = self.encoder(x, mask = attn_bias)\n",
    "\n",
    "        self.attn_weights_list = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # MHSA layer\n",
    "            # float mask adds learnable additive attention bias\n",
    "            x_in = x\n",
    "            x, attn_weights = layer(\n",
    "                x, x, x,\n",
    "                attn_mask = attn_bias,\n",
    "                average_attn_weights = True    # the paper already averages so may as well just average here...\n",
    "            )\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = x_in + x\n",
    "\n",
    "            if self.save_attn:\n",
    "                self.attn_weights_list.append(attn_weights)\n",
    "\n",
    "        x = self.lin_out(x)\n",
    "        # return x.log_softmax(dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseGraphTransformerModel_V2(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int = data.x.shape[-1],\n",
    "            pos_enc_dim: int = 16,\n",
    "            hidden_dim: int = HIDDEN,\n",
    "            num_heads: int = 1,\n",
    "            num_layers: int = 1,\n",
    "            out_dim: int = len(data.y.unique()),\n",
    "            dropout: float = 0.5,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, hidden_dim)\n",
    "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
    "        self.lin_out = Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.layers = ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.layers.append(\n",
    "                MultiheadAttention(\n",
    "                    embed_dim = hidden_dim,\n",
    "                    num_heads = num_heads,\n",
    "                    dropout = dropout\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
    "        self.dropout = dropout\n",
    "        self.save_attn = False\n",
    "\n",
    "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
    "\n",
    "        x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
    "\n",
    "        self.attn_weights_list = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            # MHSA layer\n",
    "            # float mask adds learnable additive attention bias\n",
    "            x_in = x\n",
    "            x, attn_weights = layer(\n",
    "                x, x, x,\n",
    "                average_attn_weights = True # the paper already averages so may as well just average here...\n",
    "            )\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = x_in + x\n",
    "\n",
    "            if self.save_attn:\n",
    "                self.attn_weights_list.append(attn_weights)\n",
    "\n",
    "        x = self.lin_out(x)\n",
    "\n",
    "        # return x.log_softmax(dim=-1)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z39ODu9oT2f9"
   },
   "source": [
    "# Trainers"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T00:18:15.974519Z",
     "start_time": "2025-04-27T00:18:15.974519Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.metrics import f1_score",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bgXWrDZ5V_al",
    "ExecuteTime": {
     "end_time": "2025-04-27T00:18:15.975520Z",
     "start_time": "2025-04-27T00:18:15.975520Z"
    }
   },
   "source": [
    "def Train_GCN(NUM_LAYERS, NUM_HEADS, train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    IN_DIM = train_loader.dataset[0].x.shape[1]\n",
    "    OUT_DIM = train_loader.dataset[0].y.shape[1]  # 121 for PPI\n",
    "\n",
    "    model = GNNModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM, out_dim=OUT_DIM).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.5)\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = loss_fn(out, batch.y.float())  # y is multi-label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        \n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index)          # or …dense_adj / …spd\n",
    "            probs  = torch.sigmoid(logits)                     # convert logits → probabilities\n",
    "            preds  = (probs > 0.5).cpu().numpy().astype(int)   # threshold at 0.5\n",
    "            y_true.append(batch.y.cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        return micro_f1\n",
    "\n",
    "    best_val_f1 = test_f1 = 0\n",
    "    times = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS):\n",
    "        start = time.time()\n",
    "        loss = train()\n",
    "        scheduler.step()\n",
    "        train_f1 = test(train_loader)\n",
    "        val_f1 = test(val_loader)\n",
    "        tmp_test_f1 = test(test_loader)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            test_f1 = tmp_test_f1\n",
    "\n",
    "        times.append(time.time() - start)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train: {train_f1:.4f}, Val: {val_f1:.4f}, Test: {test_f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'test_f1': test_f1\n",
    "    }, None\n",
    "\n",
    "\n",
    "def Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Get dimensions from dataset\n",
    "    IN_DIM = train_loader.dataset[0].x.shape[1]\n",
    "    OUT_DIM = train_loader.dataset[0].y.shape[1]\n",
    "\n",
    "    model = SparseGraphTransformerModel(\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        in_dim=IN_DIM,\n",
    "        out_dim=OUT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.5)\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.dense_adj)\n",
    "            loss = loss_fn(out, batch.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.dense_adj)          # or …dense_adj / …spd\n",
    "            probs  = torch.sigmoid(logits)                     # convert logits → probabilities\n",
    "            preds  = (probs > 0.5).cpu().numpy().astype(int)   # threshold at 0.5\n",
    "            y_true.append(batch.y.cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        return micro_f1\n",
    "\n",
    "    best_val_f1 = test_f1 = 0\n",
    "    times = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS):\n",
    "        start = time.time()\n",
    "        loss = train()\n",
    "        scheduler.step()\n",
    "        train_f1 = test(train_loader)\n",
    "        val_f1 = test(val_loader)\n",
    "        tmp_test_f1 = test(test_loader)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            test_f1 = tmp_test_f1\n",
    "\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train: {train_f1:.4f}, Val: {val_f1:.4f}, Test: {test_f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'test_f1': test_f1\n",
    "    }, model.cpu()\n",
    "\n",
    "def Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    IN_DIM = train_loader.dataset[0].x.shape[1]\n",
    "    OUT_DIM = train_loader.dataset[0].y.shape[1]\n",
    "\n",
    "    model = DenseGraphTransformerModel(\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        in_dim=IN_DIM,\n",
    "        out_dim=OUT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.5)\n",
    "    \n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.pos_enc, batch.dense_sp_matrix)  # batch.spd is dense_sp_matrix\n",
    "            loss = loss_fn(out, batch.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.pos_enc, batch.dense_sp_matrix)          # or …dense_adj / …spd\n",
    "            probs  = torch.sigmoid(logits)                     # convert logits → probabilities\n",
    "            preds  = (probs > 0.5).cpu().numpy().astype(int)   # threshold at 0.5\n",
    "            y_true.append(batch.y.cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        return micro_f1\n",
    "\n",
    "    best_val_f1 = test_f1 = 0\n",
    "    times = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS):\n",
    "        start = time.time()\n",
    "        loss = train()\n",
    "        scheduler.step()\n",
    "        train_f1 = test(train_loader)\n",
    "        val_f1 = test(val_loader)\n",
    "        tmp_test_f1 = test(test_loader)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            test_f1 = tmp_test_f1\n",
    "\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train: {train_f1:.4f}, Val: {val_f1:.4f}, Test: {test_f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'test_f1': test_f1\n",
    "    }, model.cpu()\n",
    "    \n",
    "    # Notes\n",
    "    # - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
    "    # - Node positional encodings are not particularly useful\n",
    "    # - Edge distance encodings are very useful\n",
    "    # - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes\n",
    "\n",
    "def Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    IN_DIM = train_loader.dataset[0].x.shape[1]\n",
    "    OUT_DIM = train_loader.dataset[0].y.shape[1]\n",
    "\n",
    "    model = DenseGraphTransformerModel_V2(\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        in_dim=IN_DIM,\n",
    "        out_dim=OUT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.5)\n",
    "    \n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.pos_enc, batch.dense_sp_matrix)  # batch.spd is dense_sp_matrix\n",
    "            loss = loss_fn(out, batch.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.pos_enc, batch.dense_sp_matrix)          # or …dense_adj / …spd\n",
    "            probs  = torch.sigmoid(logits)                     # convert logits → probabilities\n",
    "            preds  = (probs > 0.5).cpu().numpy().astype(int)   # threshold at 0.5\n",
    "            y_true.append(batch.y.cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        return micro_f1\n",
    "\n",
    "    best_val_f1 = test_f1 = 0\n",
    "    times = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS):\n",
    "        start = time.time()\n",
    "        loss = train()\n",
    "        scheduler.step()\n",
    "        train_f1 = test(train_loader)\n",
    "        val_f1 = test(val_loader)\n",
    "        tmp_test_f1 = test(test_loader)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            test_f1 = tmp_test_f1\n",
    "\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train: {train_f1:.4f}, Val: {val_f1:.4f}, Test: {test_f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'test_f1': test_f1\n",
    "    }, model.cpu()  # if model tracks attention weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veDrDxJIaO7f"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3wGJPl3YJcd"
   },
   "source": "## Training"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In original paper, the models are re-trained across multiple runs with varying train/test splits. The attention weights are then averaged across runs to get a more robust attention representation that is less affected by train/test splits. \n",
    "\n",
    "However, in the inductive setting (20 different graphs), the analysis would need to be averaged for each graph separately, making this computationally expensive.\n",
    "\n",
    "Additionally, in the inductive setting, the model may be intrinsically more stable (relative to train/test splits) due to generalizing across multiple graphs.\n",
    "\n",
    "Therefore, we only train a single model for this task and analyze each graph-model combination separately."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T00:19:04.362983Z",
     "start_time": "2025-04-27T00:18:30.040880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 10\n",
    "LR = 2e-3\n",
    "HIDDEN = 512\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "\n",
    "accuracy_statistics = {}\n",
    "models = {}\n",
    "\n",
    "# Train all models with the same loaders\n",
    "accuracy_statistics['GCN'], _ = Train_GCN(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "accuracy_statistics['SparseGraphTransformerModel'], models['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "accuracy_statistics['DenseGraphTransformerModel'], models['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "accuracy_statistics['DenseGraphTransformerModel_V2'], models['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "\n",
    "accuracy_statistics"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SparseGraphTransformerModel': {'train_f1': 0.6863282265028491,\n",
       "  'val_f1': 0.6614007148671126,\n",
       "  'test_f1': 0.6757455538923722}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Table 2: Accuracy Statistics"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T01:42:48.790765Z",
     "start_time": "2025-04-27T01:42:48.785354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Table 2 ###\n",
    "### Accuracy Statistics ###\n",
    "all_stats_df = pd.DataFrame(accuracy_statistics).T.round(2)\n",
    "all_stats_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             train_f1  val_f1  test_f1\n",
       "SparseGraphTransformerModel      0.69    0.66     0.68"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_f1</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SparseGraphTransformerModel</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Attention Graphs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 87,
   "source": [
    "def create_graph_from_attention(attention_matrix, threshold):\n",
    "    \"\"\"Creates a directed graph from an attention matrix.\"\"\"\n",
    "    attention_matrix = attention_matrix.numpy() \n",
    "    \n",
    "    # mask out weights below the threshold\n",
    "    attention_matrix = attention_matrix * (attention_matrix > threshold)\n",
    "    \n",
    "    G = nx.from_numpy_array(attention_matrix, create_using=nx.DiGraph())\n",
    "    \n",
    "    return G\n",
    "\n",
    "def get_threshold(attention_matrix, model_name=None):\n",
    "    \"\"\"Returns threshold using different percentiles based on model.\"\"\"\n",
    "    attn_values = attention_matrix.flatten().cpu().numpy()\n",
    "\n",
    "    # percentile = 99.5 if model_name == \"dt2_avg\" else 90\n",
    "    percentile_threshold = np.percentile(attn_values, 99.5)\n",
    "    mean_threshold = attn_values.mean() + 1.5 * attn_values.std()\n",
    "\n",
    "    return max(percentile_threshold, mean_threshold)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T01:11:49.929878Z",
     "start_time": "2025-04-27T01:11:41.937154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import chain\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "combined_loader = chain(train, val, test)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "graphs = {}\n",
    "\n",
    "# create attention graph for each graph\n",
    "for model_name in models.keys():\n",
    "    model = models[model_name]\n",
    "    graphs[model_name] = {} \n",
    "    \n",
    "    model.save_attn = True\n",
    "    \n",
    "    model = model.to(device)\n",
    "    for idx, g in enumerate(combined_loader):\n",
    "        g = g.to(device)\n",
    "\n",
    "        model.eval() \n",
    "        with torch.no_grad():\n",
    "            # make prediction to save attention weights \n",
    "            if model_name == 'SparseGraphTransformerModel':\n",
    "                model(g.x, g.dense_adj)\n",
    "            else:\n",
    "                model(g.x, g.pos_enc, g.dense_sp_matrix)\n",
    "            \n",
    "        # construct attention graph\n",
    "        attn_weights = model.attn_weights_list\n",
    "        \n",
    "        # perform matrix multiplication across layers to aggregate graphs \n",
    "        attn_graph = attn_weights[0]\n",
    "        \n",
    "        for attn in attn_weights[1:]:\n",
    "            attn_graph = attn_graph @ attn\n",
    "        \n",
    "        attn_graph = attn_graph.cpu()\n",
    "        \n",
    "        threshold = get_threshold(attn_graph)\n",
    "        attn_graph = create_graph_from_attention(attn_graph, threshold)\n",
    "        \n",
    "        # convert to directed to match attention format\n",
    "        og_graph = to_networkx(g, to_undirected=False)\n",
    "        \n",
    "        graphs[model_name][idx] = {\n",
    "            \"original_graph\": og_graph,\n",
    "            \"attention_graph\": attn_graph\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T02:58:10.990959Z",
     "start_time": "2025-04-27T02:58:10.985405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_metrics(G):\n",
    "    return pd.DataFrame.from_dict({\n",
    "        'degree': nx.degree_centrality(G),\n",
    "        'betweenness': nx.betweenness_centrality(G, normalized=True),\n",
    "        'closeness': nx.closeness_centrality(G),\n",
    "        'eigenvector': nx.eigenvector_centrality(G, max_iter=500, tol=1e-02),\n",
    "        'clustering': nx.clustering(G),\n",
    "        'pagerank': nx.pagerank(G, alpha=0.85)\n",
    "    })\n",
    "\n",
    "def compute_cross_metric_correlation(metrics1, metrics2):\n",
    "    metrics1 = pd.DataFrame(metrics1)\n",
    "    metrics2 = pd.DataFrame(metrics2)\n",
    "    \n",
    "    result = pd.DataFrame(index=metrics1.columns, columns=metrics2.columns)\n",
    "    \n",
    "    for col1 in metrics1.columns:\n",
    "        for col2 in metrics2.columns:\n",
    "            corr, _ = spearmanr(metrics1[col1], metrics2[col2])\n",
    "            result.loc[col1, col2] = corr\n",
    "\n",
    "    return result.astype(float)\n",
    "def corr_heatmap(corr_df, title):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(corr_df, annot=True, fmt=\".2f\", vmin=-1, vmax=1, cmap='coolwarm', cbar=True)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def topk_overlap(metrics1, metrics2, label1, label2, out_path):\n",
    "    def get_top_k_nodes(metric_dict, k=100):\n",
    "        return {\n",
    "            metric: set(sorted(metric_dict[metric].items(), key=lambda x: x[1], reverse=True)[:k])\n",
    "            for metric in metric_dict\n",
    "        }\n",
    "\n",
    "    topk_1 = get_top_k_nodes(metrics1)\n",
    "    topk_2 = get_top_k_nodes(metrics2)\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(f\"Top-100 Node Overlap Between {label1} and {label2}\\n\")\n",
    "        for metric in metrics1:\n",
    "            nodes_1 = {node for node, _ in topk_1[metric]}\n",
    "            nodes_2 = {node for node, _ in topk_2[metric]}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T02:58:12.967305Z",
     "start_time": "2025-04-27T02:58:12.963482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_graphs(model_stats, model_name):\n",
    "    total_nodes = 0\n",
    "    corr_df = None\n",
    "    for g_idx, g in tqdm(list(model_stats.items())[:5], total=len(model_stats)):\n",
    "        og_graph = g['original_graph']\n",
    "        attn_graph = g['attention_graph']\n",
    "        \n",
    "        num_nodes = og_graph.number_of_nodes()\n",
    "        total_nodes += num_nodes\n",
    "        \n",
    "        og_graph_stats = compute_metrics(og_graph)\n",
    "        attn_graph_stats = compute_metrics(attn_graph)\n",
    "        \n",
    "        corr = compute_cross_metric_correlation(og_graph_stats, attn_graph_stats)\n",
    "        corr *= num_nodes\n",
    "        if corr_df is None:\n",
    "            corr_df = corr\n",
    "        else:\n",
    "            corr_df += corr\n",
    "        \n",
    "    corr_df /= total_nodes \n",
    "    \n",
    "    corr_heatmap(corr_df, title=model_name)"
   ],
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sparse = list(graphs.keys())[0]\n",
    "analyze_graphs(graphs[sparse], sparse)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM7QF060g8m/oZks22EC8jM",
   "collapsed_sections": [
    "Fxj1ye0Y1ZwC",
    "sWeKuQTt1k_M",
    "2hMUb2yK1pEB",
    "ZXuPgGbGa2Nd",
    "670dtom5sl10"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1nYy2OQQ4os8qAJRaYQ2kwsbNjzw3zFyx",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
