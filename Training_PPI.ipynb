{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "tVe55J-aw7aC",
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:20.914980Z",
     "start_time": "2025-04-14T03:36:20.912367Z"
    }
   },
   "source": [
    "# Combining Attention values across heads - Avg\n",
    "# Combining Attention values across layers - Matrix Multiply"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRR5bUiLRZ5a"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95820,
     "status": "ok",
     "timestamp": 1714340656785,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "JLx8ARPERaaB",
    "outputId": "39a34a44-a10f-4423-81a8-ca4638185152",
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:20.918532Z",
     "start_time": "2025-04-14T03:36:20.915879Z"
    }
   },
   "source": [
    "# !pip install dgl torch_geometric torch\n",
    "\n",
    "# Install required python libraries\n",
    "import os\n",
    "\n",
    "# Install PyTorch Geometric and other libraries\n",
    "# if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "#     print(\"Installing PyTorch Geometric\")\n",
    "#     !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "#     !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "#     !pip install -q torch-geometric\n",
    "#     print(\"Installing other libraries\")\n",
    "#     !pip install networkx\n",
    "#     !pip install lovely-tensors"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7222,
     "status": "ok",
     "timestamp": 1714340664004,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "xMiwOILkRgEP",
    "outputId": "1e83443e-9aaf-4622-b4c0-33f3f2f246dd",
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:24.390620Z",
     "start_time": "2025-04-14T03:36:20.918532Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from typing import Mapping, Tuple, Sequence, List\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding, Linear, ReLU, BatchNorm1d, LayerNorm, Module, ModuleList, Sequential\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, MultiheadAttention\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import remove_self_loops, dense_to_sparse, to_dense_batch, to_dense_adj\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
    "\n",
    "# from torch_scatter import scatter, scatter_mean, scatter_max, scatter_sum\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print(\"All imports succeeded.\")\n",
    "print(\"Python version {}\".format(sys.version))\n",
    "print(\"PyTorch version {}\".format(torch.__version__))\n",
    "print(\"PyG version {}\".format(torch_geometric.__version__))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PyCharmProjects\\Graph_Attention_Interpretability\\.venv\\Lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "D:\\PyCharmProjects\\Graph_Attention_Interpretability\\.venv\\Lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports succeeded.\n",
      "Python version 3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]\n",
      "PyTorch version 2.6.0+cu118\n",
      "PyG version 2.6.1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714340664004,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "zuVTx4otRi6i",
    "outputId": "3413a2f1-30a2-413b-a451-9ad18d8e2eee",
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:24.398009Z",
     "start_time": "2025-04-14T03:36:24.390620Z"
    }
   },
   "source": [
    "# Set random seed for deterministic results\n",
    "\n",
    "def seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed(0)\n",
    "print(\"All seeds set.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All seeds set.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:24.401562Z",
     "start_time": "2025-04-14T03:36:24.399013Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Cuda available: {}\".format(torch.cuda.is_available()))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6R5GR7hRdEI"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:24.449298Z",
     "start_time": "2025-04-14T03:36:24.401562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use igraph because much faster than networkx\n",
    "import igraph as ig\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_position_info(dataset, pe_dim=16):\n",
    "    \n",
    "    processed_dataset = []\n",
    "    pe_transform = T.AddLaplacianEigenvectorPE(k=pe_dim, attr_name='pos_enc')\n",
    "\n",
    "    for data in tqdm(dataset, total=len(dataset), desc='Processing disjoint graphs'):\n",
    "        num_nodes = data.num_nodes\n",
    "        edge_index = data.edge_index\n",
    "\n",
    "        # Convert to igraph\n",
    "        edges = edge_index.t().tolist()\n",
    "        g = ig.Graph(n=num_nodes, edges=edges, directed=False)\n",
    "\n",
    "        # compute shortest path distances\n",
    "        sp_matrix = torch.tensor(g.distances(algorithm=\"johnson\"), dtype=torch.float16)\n",
    "        sp_matrix[torch.isinf(sp_matrix)] = 0\n",
    "        data.dense_sp_matrix = sp_matrix\n",
    "\n",
    "        dense_adj = to_dense_adj(edge_index, max_num_nodes=num_nodes)[0]\n",
    "        dense_adj = dense_adj + torch.eye(num_nodes, dtype=dense_adj.dtype)\n",
    "        dense_adj[dense_adj == 2] = 1  # remove double self-loops\n",
    "        data.dense_adj = dense_adj\n",
    "\n",
    "        # add Laplacian eigenvectors as positional encoding\n",
    "        data = pe_transform(data)\n",
    "\n",
    "        processed_dataset.append(data)\n",
    "\n",
    "    return processed_dataset"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44347,
     "status": "ok",
     "timestamp": 1714340708349,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "8PVfelu6ReJe",
    "outputId": "f7970243-ac97-43f7-ff9b-4fea824e91fa",
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:43.115953Z",
     "start_time": "2025-04-14T03:36:24.450301Z"
    }
   },
   "source": [
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train = add_position_info(PPI(root='/tmp/PPI', split='train'))\n",
    "val = add_position_info(PPI(root='/tmp/PPI', split='val'))\n",
    "test = add_position_info(PPI(root='/tmp/PPI', split='test'))\n",
    "\n",
    "train = DataLoader(train, batch_size=1, shuffle=True)\n",
    "val = DataLoader(val, batch_size=1)\n",
    "test = DataLoader(test, batch_size=1)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing disjoint graphs: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n",
      "Processing disjoint graphs: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it]\n",
      "Processing disjoint graphs: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:43.133504Z",
     "start_time": "2025-04-14T03:36:43.115953Z"
    }
   },
   "cell_type": "code",
   "source": "data = next(iter(train))",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:43.143109Z",
     "start_time": "2025-04-14T03:36:43.135536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ex_spd = data.dense_sp_matrix\n",
    "ex_spd.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3021, 3021])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:43.150016Z",
     "start_time": "2025-04-14T03:36:43.145623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor_memory_bytes = ex_spd.element_size() * ex_spd.numel()\n",
    "tensor_memory_MB = tensor_memory_bytes / (1024 ** 2)  # Convert to MB\n",
    "print(f\"Tensor memory usage: {tensor_memory_MB:.2f} MB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor memory usage: 17.41 MB\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:43.158342Z",
     "start_time": "2025-04-14T03:36:43.151020Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[3021, 50], edge_index=[2, 91338], y=[3021, 121], dense_sp_matrix=[3021, 3021], dense_adj=[3021, 3021], pos_enc=[3021, 16], batch=[3021], ptr=[2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPGL9tFORJCQ"
   },
   "source": [
    "## Table 1: Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1714340709202,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": -60
    },
    "id": "O34_wDE-RLre",
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:43.167005Z",
     "start_time": "2025-04-14T03:36:43.159346Z"
    }
   },
   "source": [
    "# ### Table 1 ###\n",
    "# ### Dataset Statistics ###\n",
    "# import dgl\n",
    "# Homophily_Levels = []\n",
    "# \n",
    "# for data in train:\n",
    "#   edge_index_tensor = torch.tensor(data.edge_index.cpu().numpy(), dtype=torch.long)\n",
    "#   g = dgl.graph((edge_index_tensor[0], edge_index_tensor[1]), num_nodes=data.x.shape[0])\n",
    "#   g.ndata['y'] = torch.tensor(data.y.cpu().numpy(), dtype=torch.long)\n",
    "#   Homophily_Levels.append({'Node Homophily':dgl.node_homophily(g, g.ndata['y'])*100,\n",
    "#                                 'Edge Homophily':dgl.edge_homophily(g, g.ndata['y'])*100,\n",
    "#                                 'Adjusted Homophily':dgl.adjusted_homophily(g, g.ndata['y'])*100,\n",
    "#                                 'Number of Nodes': int(g.num_nodes()),\n",
    "#                                 'Number of Edges': int(g.num_edges())\n",
    "#                                 })\n",
    "# df = pd.DataFrame(Homophily_Levels).round(1)\n",
    "# df"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JV4ZtidSENR"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AKL9b7tCSDDd",
    "ExecuteTime": {
     "end_time": "2025-04-14T07:05:10.597778Z",
     "start_time": "2025-04-14T07:05:10.561272Z"
    }
   },
   "source": [
    "# PyG example code: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py\n",
    "\n",
    "class GNNModel(Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int = data.x.shape[-1],\n",
    "            hidden_dim: int = 256,\n",
    "            num_heads: int = 1,\n",
    "            num_layers: int = 1,\n",
    "            out_dim: int = len(data.y.unique()),\n",
    "            dropout: float = 0.5,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, hidden_dim)\n",
    "        self.lin_out = Linear(hidden_dim, out_dim)\n",
    "        self.layers = ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            self.layers.append(\n",
    "                GCNConv(hidden_dim, hidden_dim)\n",
    "            )\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.lin_in(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # conv -> activation ->  dropout -> residual\n",
    "            x_in = x\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = x_in + x\n",
    "\n",
    "        x = self.lin_out(x)\n",
    "        # return x.log_softmax(dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SparseGraphTransformerModel(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int = data.x.shape[-1],\n",
    "            hidden_dim: int = 256,\n",
    "            num_heads: int = 1,\n",
    "            num_layers: int = 1,\n",
    "            out_dim: int = len(data.y.unique()),\n",
    "            dropout: float = 0.5,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, hidden_dim)\n",
    "        self.lin_out = Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.layers = ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.layers.append(\n",
    "                MultiheadAttention(\n",
    "                    embed_dim = hidden_dim,\n",
    "                    num_heads = num_heads,\n",
    "                    dropout = dropout\n",
    "                )\n",
    "            )\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, dense_adj):\n",
    "\n",
    "        x = self.lin_in(x)\n",
    "\n",
    "        self.attn_weights_list = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_in = x\n",
    "            x, attn_weights = layer(\n",
    "                x, x, x,\n",
    "                attn_mask = ~dense_adj.bool(),\n",
    "                average_attn_weights = False\n",
    "            )\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = x_in + x\n",
    "\n",
    "            self.attn_weights_list.append(attn_weights)\n",
    "\n",
    "        x = self.lin_out(x)\n",
    "\n",
    "        # return x.log_softmax(dim=-1)\n",
    "        return x\n",
    "\n",
    "class DenseGraphTransformerModel(Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int = data.x.shape[-1],\n",
    "            pos_enc_dim: int = 16,\n",
    "            hidden_dim: int = 256,\n",
    "            num_heads: int = 1,\n",
    "            num_layers: int = 1,\n",
    "            out_dim: int = len(data.y.unique()),\n",
    "            dropout: float = 0.5,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, hidden_dim)\n",
    "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
    "        self.lin_out = Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.layers = ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.layers.append(\n",
    "                MultiheadAttention(\n",
    "                    embed_dim = hidden_dim,\n",
    "                    num_heads = num_heads,\n",
    "                    dropout = dropout\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
    "\n",
    "        # x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
    "        x = self.lin_in(x)  # no node positional encoding\n",
    "\n",
    "        # attention bias\n",
    "        # [i, j] -> inverse of shortest path distance b/w node i and j\n",
    "        # diagonals -> self connection, set to 0\n",
    "        # disconnected nodes -> -1\n",
    "        attn_bias = self.attn_bias_scale * torch.nan_to_num(\n",
    "            (1 / (torch.nan_to_num(dense_sp_matrix, nan=-1, posinf=-1, neginf=-1))),\n",
    "            nan=0, posinf=0, neginf=0)\n",
    "        #attn_bias = torch.ones_like(attn_bias)\n",
    "\n",
    "        # TransformerEncoder\n",
    "        # x = self.encoder(x, mask = attn_bias)\n",
    "\n",
    "        self.attn_weights_list = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # MHSA layer\n",
    "            # float mask adds learnable additive attention bias\n",
    "            x_in = x\n",
    "            x, attn_weights = layer(\n",
    "                x, x, x,\n",
    "                attn_mask = attn_bias,\n",
    "                average_attn_weights = False\n",
    "            )\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = x_in + x\n",
    "\n",
    "            self.attn_weights_list.append(attn_weights)\n",
    "\n",
    "        x = self.lin_out(x)\n",
    "        # return x.log_softmax(dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DenseGraphTransformerModel_V2(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int = data.x.shape[-1],\n",
    "            pos_enc_dim: int = 16,\n",
    "            hidden_dim: int = 256,\n",
    "            num_heads: int = 1,\n",
    "            num_layers: int = 1,\n",
    "            out_dim: int = len(data.y.unique()),\n",
    "            dropout: float = 0.5,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_in = Linear(in_dim, hidden_dim)\n",
    "        self.lin_pos_enc = Linear(pos_enc_dim, hidden_dim)\n",
    "        self.lin_out = Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.layers = ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.layers.append(\n",
    "                MultiheadAttention(\n",
    "                    embed_dim = hidden_dim,\n",
    "                    num_heads = num_heads,\n",
    "                    dropout = dropout\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.attn_bias_scale = torch.nn.Parameter(torch.tensor([10.0]))  # controls how much we initially bias our model to nearby nodes\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, pos_enc, dense_sp_matrix):\n",
    "\n",
    "        x = self.lin_in(x) + self.lin_pos_enc(pos_enc)\n",
    "\n",
    "        self.attn_weights_list = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            # MHSA layer\n",
    "            # float mask adds learnable additive attention bias\n",
    "            x_in = x\n",
    "            x, attn_weights = layer(\n",
    "                x, x, x,\n",
    "                average_attn_weights = False\n",
    "            )\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            x = x_in + x\n",
    "\n",
    "            self.attn_weights_list.append(attn_weights)\n",
    "\n",
    "        x = self.lin_out(x)\n",
    "\n",
    "        # return x.log_softmax(dim=-1)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z39ODu9oT2f9"
   },
   "source": [
    "# Trainers"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:43.330724Z",
     "start_time": "2025-04-14T03:36:43.206314Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.metrics import f1_score",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bgXWrDZ5V_al",
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:43.348217Z",
     "start_time": "2025-04-14T03:36:43.331727Z"
    }
   },
   "source": [
    "def Train_GCN(NUM_LAYERS, NUM_HEADS, train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    IN_DIM = train_loader.dataset[0].x.shape[1]\n",
    "    OUT_DIM = train_loader.dataset[0].y.shape[1]  # 121 for PPI\n",
    "\n",
    "    model = GNNModel(num_layers=NUM_LAYERS, num_heads=NUM_HEADS, in_dim=IN_DIM, out_dim=OUT_DIM).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.5)\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = loss_fn(out, batch.y.float())  # y is multi-label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        \n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.edge_index)          # or …dense_adj / …spd\n",
    "            probs  = torch.sigmoid(logits)                     # convert logits → probabilities\n",
    "            preds  = (probs > 0.5).cpu().numpy().astype(int)   # threshold at 0.5\n",
    "            y_true.append(batch.y.cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        return micro_f1\n",
    "\n",
    "    best_val_f1 = test_f1 = 0\n",
    "    times = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS):\n",
    "        start = time.time()\n",
    "        loss = train()\n",
    "        scheduler.step()\n",
    "        train_f1 = test(train_loader)\n",
    "        val_f1 = test(val_loader)\n",
    "        tmp_test_f1 = test(test_loader)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            test_f1 = tmp_test_f1\n",
    "\n",
    "        times.append(time.time() - start)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train: {train_f1:.4f}, Val: {val_f1:.4f}, Test: {test_f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'test_f1': test_f1\n",
    "    }, None\n",
    "\n",
    "\n",
    "def Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Get dimensions from dataset\n",
    "    IN_DIM = train_loader.dataset[0].x.shape[1]\n",
    "    OUT_DIM = train_loader.dataset[0].y.shape[1]\n",
    "\n",
    "    model = SparseGraphTransformerModel(\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        in_dim=IN_DIM,\n",
    "        out_dim=OUT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.5)\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.dense_adj)\n",
    "            loss = loss_fn(out, batch.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.dense_adj)          # or …dense_adj / …spd\n",
    "            probs  = torch.sigmoid(logits)                     # convert logits → probabilities\n",
    "            preds  = (probs > 0.5).cpu().numpy().astype(int)   # threshold at 0.5\n",
    "            y_true.append(batch.y.cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        return micro_f1\n",
    "\n",
    "    best_val_f1 = test_f1 = 0\n",
    "    times = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS):\n",
    "        start = time.time()\n",
    "        loss = train()\n",
    "        scheduler.step()\n",
    "        train_f1 = test(train_loader)\n",
    "        val_f1 = test(val_loader)\n",
    "        tmp_test_f1 = test(test_loader)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            test_f1 = tmp_test_f1\n",
    "\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train: {train_f1:.4f}, Val: {val_f1:.4f}, Test: {test_f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'test_f1': test_f1\n",
    "    }, model.attn_weights_list  # assuming your model exposes attention weights this way\n",
    "\n",
    "def Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    IN_DIM = train_loader.dataset[0].x.shape[1]\n",
    "    OUT_DIM = train_loader.dataset[0].y.shape[1]\n",
    "\n",
    "    model = DenseGraphTransformerModel(\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        in_dim=IN_DIM,\n",
    "        out_dim=OUT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.5)\n",
    "    \n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.pos_enc, batch.dense_sp_matrix)  # batch.spd is dense_sp_matrix\n",
    "            loss = loss_fn(out, batch.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.pos_enc, batch.dense_sp_matrix)          # or …dense_adj / …spd\n",
    "            probs  = torch.sigmoid(logits)                     # convert logits → probabilities\n",
    "            preds  = (probs > 0.5).cpu().numpy().astype(int)   # threshold at 0.5\n",
    "            y_true.append(batch.y.cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        return micro_f1\n",
    "\n",
    "    best_val_f1 = test_f1 = 0\n",
    "    times = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS):\n",
    "        start = time.time()\n",
    "        loss = train()\n",
    "        scheduler.step()\n",
    "        train_f1 = test(train_loader)\n",
    "        val_f1 = test(val_loader)\n",
    "        tmp_test_f1 = test(test_loader)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            test_f1 = tmp_test_f1\n",
    "\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train: {train_f1:.4f}, Val: {val_f1:.4f}, Test: {test_f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'test_f1': test_f1\n",
    "    }, model.attn_weights_list  # if model tracks attention weights\n",
    "    \n",
    "    # Notes\n",
    "    # - Dense Transformer needs to be trained for a bit longer to reach low loss value\n",
    "    # - Node positional encodings are not particularly useful\n",
    "    # - Edge distance encodings are very useful\n",
    "    # - Since Cora is highly homophilic, it is important to bias the attention towards nearby nodes\n",
    "\n",
    "def Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, train_loader, val_loader, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    IN_DIM = train_loader.dataset[0].x.shape[1]\n",
    "    OUT_DIM = train_loader.dataset[0].y.shape[1]\n",
    "\n",
    "    model = DenseGraphTransformerModel_V2(\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        in_dim=IN_DIM,\n",
    "        out_dim=OUT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=.5)\n",
    "    \n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.pos_enc, batch.dense_sp_matrix)  # batch.spd is dense_sp_matrix\n",
    "            loss = loss_fn(out, batch.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x, batch.pos_enc, batch.dense_sp_matrix)          # or …dense_adj / …spd\n",
    "            probs  = torch.sigmoid(logits)                     # convert logits → probabilities\n",
    "            preds  = (probs > 0.5).cpu().numpy().astype(int)   # threshold at 0.5\n",
    "            y_true.append(batch.y.cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = np.vstack(y_pred)\n",
    "        micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        return micro_f1\n",
    "\n",
    "    best_val_f1 = test_f1 = 0\n",
    "    times = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS):\n",
    "        start = time.time()\n",
    "        loss = train()\n",
    "        scheduler.step()\n",
    "        train_f1 = test(train_loader)\n",
    "        val_f1 = test(val_loader)\n",
    "        tmp_test_f1 = test(test_loader)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            test_f1 = tmp_test_f1\n",
    "\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss:.4f}, Train: {train_f1:.4f}, Val: {val_f1:.4f}, Test: {test_f1:.4f}')\n",
    "\n",
    "    return {\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'test_f1': test_f1\n",
    "    }, model.attn_weights_list  # if model tracks attention weights"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veDrDxJIaO7f"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3wGJPl3YJcd"
   },
   "source": [
    "## Training: 1 Layer, 1 Head"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T05:13:36.165317Z",
     "start_time": "2025-04-14T05:13:36.161803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 300\n",
    "LR = 2e-3"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229201,
     "status": "ok",
     "timestamp": 1710738973707,
     "user": {
      "displayName": "Batu El",
      "userId": "11666366648103508022"
     },
     "user_tz": 0
    },
    "id": "y6GoLurKW57q",
    "outputId": "5a2e6a7c-8104-4d38-cdff-fb422102fd9b",
    "ExecuteTime": {
     "end_time": "2025-04-14T03:36:43.366001Z",
     "start_time": "2025-04-14T03:36:43.363493Z"
    }
   },
   "source": [
    "# NUM_LAYERS = 1\n",
    "# NUM_HEADS = 1\n",
    "# NUM_RUNS = 1\n",
    "# \n",
    "# run_stats = {}\n",
    "# \n",
    "# for run in tqdm(range(NUM_RUNS), total=NUM_RUNS, desc='Runs'):\n",
    "#     accuracy_statistics = {}\n",
    "#     attn_weights = {}\n",
    "# \n",
    "#     # Train all models with the same loaders\n",
    "#     # accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "#     accuracy_statistics['SparseGraphTransformerModel'], attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "#     accuracy_statistics['DenseGraphTransformerModel'], attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "#     # accuracy_statistics['DenseGraphTransformerModel_V2'], attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "# \n",
    "#     # Convert attention weights to CPU tensors\n",
    "#     for key in attn_weights.keys():\n",
    "#         if attn_weights[key]:\n",
    "#             attn_weights[key] = torch.stack(attn_weights[key]).cpu()\n",
    "# \n",
    "#     run_stats[run] = {'f1': accuracy_statistics, 'attentions': attn_weights}\n",
    "#     \n",
    "# run_stats"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training: 1 Layer, 2 Heads"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T07:45:05.393731Z",
     "start_time": "2025-04-14T07:05:19.959813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "NUM_RUNS = 1\n",
    "\n",
    "run_stats = {}\n",
    "\n",
    "for run in tqdm(range(NUM_RUNS), total=NUM_RUNS, desc='Runs'):\n",
    "    accuracy_statistics = {}\n",
    "    attn_weights = {}\n",
    "\n",
    "    # Train all models with the same loaders\n",
    "    accuracy_statistics['GCN'], attn_weights['GCN'] = Train_GCN(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "    accuracy_statistics['SparseGraphTransformerModel'], attn_weights['SparseGraphTransformerModel'] = Train_SparseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "    accuracy_statistics['DenseGraphTransformerModel'], attn_weights['DenseGraphTransformerModel'] = Train_DenseGraphTransformerModel(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "    accuracy_statistics['DenseGraphTransformerModel_V2'], attn_weights['DenseGraphTransformerModel_V2'] = Train_DenseGraphTransformerModel_V2(NUM_LAYERS, NUM_HEADS, train, val, test)\n",
    "\n",
    "    # Convert attention weights to CPU tensors\n",
    "    for key in attn_weights.keys():\n",
    "        if attn_weights[key]:\n",
    "            attn_weights[key] = torch.stack(attn_weights[key]).cpu()\n",
    "\n",
    "    run_stats[run] = {'f1': accuracy_statistics, 'attentions': attn_weights}\n",
    "    \n",
    "run_stats"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Runs:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.4609, Train: 0.5768, Val: 0.5533, Test: 0.5878\n",
      "Epoch 20, Loss: 0.4218, Train: 0.6679, Val: 0.6399, Test: 0.6596\n",
      "Epoch 30, Loss: 0.3941, Train: 0.7238, Val: 0.6890, Test: 0.7111\n",
      "Epoch 40, Loss: 0.3799, Train: 0.7367, Val: 0.6999, Test: 0.7227\n",
      "Epoch 50, Loss: 0.3714, Train: 0.7450, Val: 0.7067, Test: 0.7356\n",
      "Epoch 60, Loss: 0.3649, Train: 0.7556, Val: 0.7184, Test: 0.7462\n",
      "Epoch 70, Loss: 0.3582, Train: 0.7662, Val: 0.7271, Test: 0.7521\n",
      "Epoch 80, Loss: 0.3546, Train: 0.7697, Val: 0.7301, Test: 0.7580\n",
      "Epoch 90, Loss: 0.3528, Train: 0.7737, Val: 0.7338, Test: 0.7595\n",
      "Epoch 100, Loss: 0.3503, Train: 0.7762, Val: 0.7366, Test: 0.7623\n",
      "Epoch 110, Loss: 0.3491, Train: 0.7784, Val: 0.7388, Test: 0.7649\n",
      "Epoch 120, Loss: 0.3466, Train: 0.7789, Val: 0.7389, Test: 0.7653\n",
      "Epoch 130, Loss: 0.3458, Train: 0.7806, Val: 0.7404, Test: 0.7672\n",
      "Epoch 140, Loss: 0.3455, Train: 0.7812, Val: 0.7412, Test: 0.7681\n",
      "Epoch 150, Loss: 0.3443, Train: 0.7811, Val: 0.7410, Test: 0.7695\n",
      "Epoch 160, Loss: 0.3437, Train: 0.7831, Val: 0.7431, Test: 0.7695\n",
      "Epoch 170, Loss: 0.3436, Train: 0.7847, Val: 0.7446, Test: 0.7700\n",
      "Epoch 180, Loss: 0.3428, Train: 0.7838, Val: 0.7438, Test: 0.7703\n",
      "Epoch 190, Loss: 0.3431, Train: 0.7842, Val: 0.7440, Test: 0.7703\n",
      "Epoch 200, Loss: 0.3426, Train: 0.7846, Val: 0.7445, Test: 0.7706\n",
      "Epoch 210, Loss: 0.3430, Train: 0.7847, Val: 0.7448, Test: 0.7706\n",
      "Epoch 220, Loss: 0.3430, Train: 0.7856, Val: 0.7457, Test: 0.7713\n",
      "Epoch 230, Loss: 0.3417, Train: 0.7849, Val: 0.7448, Test: 0.7713\n",
      "Epoch 240, Loss: 0.3419, Train: 0.7855, Val: 0.7456, Test: 0.7713\n",
      "Epoch 250, Loss: 0.3431, Train: 0.7852, Val: 0.7453, Test: 0.7713\n",
      "Epoch 260, Loss: 0.3420, Train: 0.7857, Val: 0.7458, Test: 0.7715\n",
      "Epoch 270, Loss: 0.3419, Train: 0.7855, Val: 0.7456, Test: 0.7715\n",
      "Epoch 280, Loss: 0.3420, Train: 0.7856, Val: 0.7456, Test: 0.7715\n",
      "Epoch 290, Loss: 0.3421, Train: 0.7855, Val: 0.7455, Test: 0.7715\n",
      "Epoch 10, Loss: 0.4275, Train: 0.6972, Val: 0.6723, Test: 0.6883\n",
      "Epoch 20, Loss: 0.3996, Train: 0.7421, Val: 0.7155, Test: 0.7307\n",
      "Epoch 30, Loss: 0.3812, Train: 0.7687, Val: 0.7406, Test: 0.7584\n",
      "Epoch 40, Loss: 0.3563, Train: 0.7915, Val: 0.7630, Test: 0.7804\n",
      "Epoch 50, Loss: 0.3514, Train: 0.7999, Val: 0.7699, Test: 0.7902\n",
      "Epoch 60, Loss: 0.3423, Train: 0.8094, Val: 0.7790, Test: 0.7995\n",
      "Epoch 70, Loss: 0.3308, Train: 0.8192, Val: 0.7890, Test: 0.8082\n",
      "Epoch 80, Loss: 0.3271, Train: 0.8231, Val: 0.7932, Test: 0.8137\n",
      "Epoch 90, Loss: 0.3236, Train: 0.8293, Val: 0.7994, Test: 0.8187\n",
      "Epoch 100, Loss: 0.3184, Train: 0.8353, Val: 0.8056, Test: 0.8253\n",
      "Epoch 110, Loss: 0.3168, Train: 0.8356, Val: 0.8052, Test: 0.8254\n",
      "Epoch 120, Loss: 0.3136, Train: 0.8382, Val: 0.8079, Test: 0.8280\n",
      "Epoch 130, Loss: 0.3109, Train: 0.8397, Val: 0.8097, Test: 0.8304\n",
      "Epoch 140, Loss: 0.3112, Train: 0.8417, Val: 0.8115, Test: 0.8310\n",
      "Epoch 150, Loss: 0.3086, Train: 0.8426, Val: 0.8121, Test: 0.8326\n",
      "Epoch 160, Loss: 0.3082, Train: 0.8433, Val: 0.8130, Test: 0.8337\n",
      "Epoch 170, Loss: 0.3074, Train: 0.8442, Val: 0.8141, Test: 0.8339\n",
      "Epoch 180, Loss: 0.3066, Train: 0.8445, Val: 0.8139, Test: 0.8343\n",
      "Epoch 190, Loss: 0.3058, Train: 0.8449, Val: 0.8144, Test: 0.8346\n",
      "Epoch 200, Loss: 0.3070, Train: 0.8452, Val: 0.8148, Test: 0.8351\n",
      "Epoch 210, Loss: 0.3057, Train: 0.8462, Val: 0.8158, Test: 0.8357\n",
      "Epoch 220, Loss: 0.3061, Train: 0.8460, Val: 0.8155, Test: 0.8357\n",
      "Epoch 230, Loss: 0.3052, Train: 0.8463, Val: 0.8158, Test: 0.8358\n",
      "Epoch 240, Loss: 0.3053, Train: 0.8463, Val: 0.8160, Test: 0.8358\n",
      "Epoch 250, Loss: 0.3054, Train: 0.8463, Val: 0.8160, Test: 0.8359\n",
      "Epoch 260, Loss: 0.3044, Train: 0.8465, Val: 0.8160, Test: 0.8361\n",
      "Epoch 270, Loss: 0.3051, Train: 0.8467, Val: 0.8163, Test: 0.8364\n",
      "Epoch 280, Loss: 0.3045, Train: 0.8466, Val: 0.8162, Test: 0.8364\n",
      "Epoch 290, Loss: 0.3040, Train: 0.8465, Val: 0.8162, Test: 0.8364\n",
      "Epoch 10, Loss: 0.4385, Train: 0.6823, Val: 0.6553, Test: 0.6715\n",
      "Epoch 20, Loss: 0.4037, Train: 0.7068, Val: 0.6691, Test: 0.7122\n",
      "Epoch 30, Loss: 0.3849, Train: 0.7610, Val: 0.7282, Test: 0.7476\n",
      "Epoch 40, Loss: 0.3589, Train: 0.7768, Val: 0.7397, Test: 0.7663\n",
      "Epoch 50, Loss: 0.3487, Train: 0.7954, Val: 0.7595, Test: 0.7852\n",
      "Epoch 60, Loss: 0.3426, Train: 0.8011, Val: 0.7642, Test: 0.7891\n",
      "Epoch 70, Loss: 0.3290, Train: 0.8129, Val: 0.7745, Test: 0.7994\n",
      "Epoch 80, Loss: 0.3245, Train: 0.8187, Val: 0.7805, Test: 0.8078\n",
      "Epoch 90, Loss: 0.3209, Train: 0.8252, Val: 0.7893, Test: 0.8124\n",
      "Epoch 100, Loss: 0.3150, Train: 0.8312, Val: 0.7943, Test: 0.8169\n",
      "Epoch 110, Loss: 0.3130, Train: 0.8313, Val: 0.7944, Test: 0.8187\n",
      "Epoch 120, Loss: 0.3099, Train: 0.8361, Val: 0.7992, Test: 0.8224\n",
      "Epoch 130, Loss: 0.3079, Train: 0.8372, Val: 0.8005, Test: 0.8243\n",
      "Epoch 140, Loss: 0.3065, Train: 0.8385, Val: 0.8021, Test: 0.8257\n",
      "Epoch 150, Loss: 0.3055, Train: 0.8393, Val: 0.8025, Test: 0.8264\n",
      "Epoch 160, Loss: 0.3034, Train: 0.8387, Val: 0.8020, Test: 0.8264\n",
      "Epoch 170, Loss: 0.3040, Train: 0.8392, Val: 0.8020, Test: 0.8270\n",
      "Epoch 180, Loss: 0.3023, Train: 0.8409, Val: 0.8045, Test: 0.8281\n",
      "Epoch 190, Loss: 0.3028, Train: 0.8410, Val: 0.8043, Test: 0.8281\n",
      "Epoch 200, Loss: 0.3018, Train: 0.8418, Val: 0.8052, Test: 0.8281\n",
      "Epoch 210, Loss: 0.3011, Train: 0.8419, Val: 0.8052, Test: 0.8281\n",
      "Epoch 220, Loss: 0.3021, Train: 0.8422, Val: 0.8056, Test: 0.8281\n",
      "Epoch 230, Loss: 0.3013, Train: 0.8419, Val: 0.8052, Test: 0.8281\n",
      "Epoch 240, Loss: 0.3022, Train: 0.8426, Val: 0.8060, Test: 0.8285\n",
      "Epoch 250, Loss: 0.3005, Train: 0.8426, Val: 0.8060, Test: 0.8285\n",
      "Epoch 260, Loss: 0.3005, Train: 0.8426, Val: 0.8059, Test: 0.8285\n",
      "Epoch 270, Loss: 0.3014, Train: 0.8426, Val: 0.8060, Test: 0.8284\n",
      "Epoch 280, Loss: 0.3011, Train: 0.8430, Val: 0.8063, Test: 0.8284\n",
      "Epoch 290, Loss: 0.3013, Train: 0.8430, Val: 0.8064, Test: 0.8284\n",
      "Epoch 10, Loss: 0.5223, Train: 0.4789, Val: 0.4673, Test: 0.4825\n",
      "Epoch 20, Loss: 0.5076, Train: 0.5016, Val: 0.4879, Test: 0.5058\n",
      "Epoch 30, Loss: 0.5001, Train: 0.5056, Val: 0.4886, Test: 0.5212\n",
      "Epoch 40, Loss: 0.4874, Train: 0.5291, Val: 0.5104, Test: 0.5273\n",
      "Epoch 50, Loss: 0.4832, Train: 0.5333, Val: 0.5122, Test: 0.5293\n",
      "Epoch 60, Loss: 0.4816, Train: 0.5438, Val: 0.5254, Test: 0.5353\n",
      "Epoch 70, Loss: 0.4766, Train: 0.5364, Val: 0.5174, Test: 0.5353\n",
      "Epoch 80, Loss: 0.4753, Train: 0.5404, Val: 0.5176, Test: 0.5418\n",
      "Epoch 90, Loss: 0.4744, Train: 0.5421, Val: 0.5185, Test: 0.5418\n",
      "Epoch 100, Loss: 0.4722, Train: 0.5459, Val: 0.5238, Test: 0.5418\n",
      "Epoch 110, Loss: 0.4709, Train: 0.5487, Val: 0.5270, Test: 0.5418\n",
      "Epoch 120, Loss: 0.4705, Train: 0.5458, Val: 0.5239, Test: 0.5418\n",
      "Epoch 130, Loss: 0.4697, Train: 0.5466, Val: 0.5241, Test: 0.5418\n",
      "Epoch 140, Loss: 0.4692, Train: 0.5487, Val: 0.5259, Test: 0.5418\n",
      "Epoch 150, Loss: 0.4690, Train: 0.5468, Val: 0.5246, Test: 0.5418\n",
      "Epoch 160, Loss: 0.4681, Train: 0.5493, Val: 0.5259, Test: 0.5418\n",
      "Epoch 170, Loss: 0.4683, Train: 0.5485, Val: 0.5266, Test: 0.5418\n",
      "Epoch 180, Loss: 0.4677, Train: 0.5495, Val: 0.5262, Test: 0.5418\n",
      "Epoch 190, Loss: 0.4678, Train: 0.5493, Val: 0.5258, Test: 0.5418\n",
      "Epoch 200, Loss: 0.4677, Train: 0.5511, Val: 0.5284, Test: 0.5418\n",
      "Epoch 210, Loss: 0.4675, Train: 0.5503, Val: 0.5270, Test: 0.5418\n",
      "Epoch 220, Loss: 0.4676, Train: 0.5498, Val: 0.5272, Test: 0.5418\n",
      "Epoch 230, Loss: 0.4676, Train: 0.5503, Val: 0.5272, Test: 0.5418\n",
      "Epoch 240, Loss: 0.4675, Train: 0.5500, Val: 0.5269, Test: 0.5418\n",
      "Epoch 250, Loss: 0.4674, Train: 0.5498, Val: 0.5269, Test: 0.5418\n",
      "Epoch 260, Loss: 0.4671, Train: 0.5501, Val: 0.5270, Test: 0.5418\n",
      "Epoch 270, Loss: 0.4674, Train: 0.5501, Val: 0.5270, Test: 0.5418\n",
      "Epoch 280, Loss: 0.4670, Train: 0.5500, Val: 0.5269, Test: 0.5418\n",
      "Epoch 290, Loss: 0.4670, Train: 0.5501, Val: 0.5268, Test: 0.5418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Runs: 100%|██████████| 1/1 [39:45<00:00, 2385.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'f1': {'GCN': {'train_f1': 0.7854119414722556,\n",
       "    'val_f1': 0.7454335609373857,\n",
       "    'test_f1': 0.771457050590865},\n",
       "   'SparseGraphTransformerModel': {'train_f1': 0.8465170344978604,\n",
       "    'val_f1': 0.8161225530135022,\n",
       "    'test_f1': 0.8362625043336547},\n",
       "   'DenseGraphTransformerModel': {'train_f1': 0.8431454813962813,\n",
       "    'val_f1': 0.8066145906362654,\n",
       "    'test_f1': 0.8284330456124078},\n",
       "   'DenseGraphTransformerModel_V2': {'train_f1': 0.5501579516752433,\n",
       "    'val_f1': 0.5271508220627498,\n",
       "    'test_f1': 0.5417549572166261}},\n",
       "  'attentions': {'GCN': None,\n",
       "   'SparseGraphTransformerModel': tensor[4, 4, 2300, 2300] n=84640000 (0.3Gb) x∈[0., 1.000] μ=0.000 σ=0.012 grad ToCopyBackward0,\n",
       "   'DenseGraphTransformerModel': tensor[4, 4, 2300, 2300] n=84640000 (0.3Gb) x∈[2.019e-32, 0.999] μ=0.000 σ=0.007 grad ToCopyBackward0,\n",
       "   'DenseGraphTransformerModel_V2': tensor[4, 4, 2300, 2300] n=84640000 (0.3Gb) x∈[0., 1.000] μ=0.000 σ=0.002 grad ToCopyBackward0}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWeKuQTt1k_M"
   },
   "source": [
    "## Training: 2 Layers, 1 Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hMUb2yK1pEB"
   },
   "source": [
    "## Training: 2 Layers, 2 Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXuPgGbGa2Nd"
   },
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "670dtom5sl10"
   },
   "source": [
    "## Table 2: Accuracy Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZNkbWu7zH06U",
    "ExecuteTime": {
     "end_time": "2025-04-14T03:55:05.673593Z",
     "start_time": "2025-04-14T03:55:05.271757Z"
    }
   },
   "source": [
    "### Table 2 ###\n",
    "### Accuracy Statistics ###\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "all_stats_df = {}\n",
    "for data_key in all_stats:\n",
    "  run_stats = all_stats[data_key]\n",
    "  table1 = pd.concat({key : pd.DataFrame(run_stats[key]['accuracy']) for key in run_stats}, axis=0)\n",
    "  table1_train = pd.concat({'mean': table1.mean(level=1, axis=0).loc['train_acc'], 'std':table1.std(level=1).loc['train_acc']}, axis=1)\n",
    "  table1_test = pd.concat({'mean': table1.mean(level=1, axis=0).loc['test_acc'], 'std':table1.std(level=1).loc['test_acc']}, axis=1)\n",
    "  # table1 = pd.concat({'Train': table1_train, 'Test': table1_test}, axis=1)\n",
    "  table1 = table1_test\n",
    "  all_stats_df[data_key] = table1\n",
    "pd.concat(all_stats_df, axis=1).round(2)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m pd.set_option(\u001B[33m'\u001B[39m\u001B[33mdisplay.max_columns\u001B[39m\u001B[33m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m      5\u001B[39m all_stats_df = {}\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m data_key \u001B[38;5;129;01min\u001B[39;00m \u001B[43mall_stats\u001B[49m:\n\u001B[32m      7\u001B[39m   run_stats = all_stats[data_key]\n\u001B[32m      8\u001B[39m   table1 = pd.concat({key : pd.DataFrame(run_stats[key][\u001B[33m'\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m run_stats}, axis=\u001B[32m0\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'all_stats' is not defined"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM7QF060g8m/oZks22EC8jM",
   "collapsed_sections": [
    "Fxj1ye0Y1ZwC",
    "sWeKuQTt1k_M",
    "2hMUb2yK1pEB",
    "ZXuPgGbGa2Nd",
    "670dtom5sl10"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1nYy2OQQ4os8qAJRaYQ2kwsbNjzw3zFyx",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
